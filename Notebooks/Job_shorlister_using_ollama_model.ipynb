{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccb6f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain-community chromadb sentence-transformers faiss-cpu requests pillow python-multipart opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8e70d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-docx in /opt/homebrew/lib/python3.11/site-packages (1.2.0)\n",
      "Requirement already satisfied: lxml>=3.1.0 in /opt/homebrew/lib/python3.11/site-packages (from python-docx) (6.0.2)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in /Users/siddharthshailendra/Library/Python/3.11/lib/python/site-packages (from python-docx) (4.15.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/opt/homebrew/opt/python@3.11/bin/python3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "import time\n",
    "import PyPDF2\n",
    "import docx\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98d02657",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightweightOllama:\n",
    "    def __init__(self, model_name=\"phi3:mini\"):\n",
    "        self.model_name = model_name\n",
    "        self.base_url = \"http://localhost:11434\"\n",
    "    \n",
    "    def invoke(self, prompt):\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                f\"{self.base_url}/api/generate\",\n",
    "                json={\n",
    "                    \"model\": self.model_name,\n",
    "                    \"prompt\": prompt,\n",
    "                    \"stream\": False,\n",
    "                    \"options\": {\n",
    "                        \"temperature\": 0.1,\n",
    "                        \"num_predict\": 200,\n",
    "                        \"top_k\": 20\n",
    "                    }\n",
    "                },\n",
    "                timeout=30\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                result = response.json().get(\"response\", \"\")\n",
    "                if \"@@@@\" in result or \"####\" in result:\n",
    "                    return \"Model response contained invalid characters. Please try a different model.\"\n",
    "                return result\n",
    "            else:\n",
    "                return f\"Error: {response.status_code}\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            return f\"Connection error: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5738a5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_lightweight_model():\n",
    "    try:\n",
    "        response = requests.get(\"http://localhost:11434/api/tags\", timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            models = response.json().get('models', [])\n",
    "            available_models = [model['name'] for model in models]\n",
    "            print(\"Available models:\", available_models)\n",
    "            preferred_models = ['phi3:mini', 'gemma2:2b', 'qwen2:0.5b', 'phi:latest', 'llama2:latest']\n",
    "            \n",
    "            for model in preferred_models:\n",
    "                if model in available_models:\n",
    "                    print(f\"‚úÖ Using {model} (lightweight)\")\n",
    "                    return model\n",
    "            \n",
    "            if available_models:\n",
    "                print(f\"‚ö†Ô∏è Using {available_models[0]} (fallback)\")\n",
    "                return available_models[0]\n",
    "                \n",
    "        return None\n",
    "    except:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d74205c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models: ['llava:latest', 'qwen2:0.5b', 'gemma2:2b', 'phi3:mini', 'phi:latest', 'llama2:latest']\n",
      "‚úÖ Using phi3:mini (lightweight)\n"
     ]
    }
   ],
   "source": [
    "model_name = get_best_lightweight_model()\n",
    "if model_name:\n",
    "    llm = LightweightOllama(model_name)\n",
    "else:\n",
    "    class SimpleLLM:\n",
    "        def invoke(self, prompt):\n",
    "            return \"Please install a lightweight model: ollama pull phi3:mini\"\n",
    "    llm = SimpleLLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0fcf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResumeParser:\n",
    "    def __init__(self):\n",
    "        self.skills_keywords = [\n",
    "            'python', 'java', 'javascript', 'sql', 'machine learning', 'deep learning',\n",
    "            'tensorflow', 'pytorch', 'react', 'node.js', 'aws', 'azure', 'docker',\n",
    "            'kubernetes', 'git', 'ci/cd', 'data analysis', 'tableau', 'power bi',\n",
    "            'mongodb', 'postgresql', 'mysql', 'html', 'css', 'rest api', 'graphql',\n",
    "            'agile', 'scrum', 'project management', 'leadership', 'communication',\n",
    "            'data science', 'artificial intelligence', 'ai', 'ml', 'nlp', 'computer vision',\n",
    "            'big data', 'spark', 'hadoop', 'kafka', 'redis', 'elasticsearch',\n",
    "            'jenkins', 'ansible', 'terraform', 'gcp', 'cloud', 'devops',\n",
    "            'frontend', 'backend', 'full stack', 'mobile', 'ios', 'android',\n",
    "            'flutter', 'react native', 'vue', 'angular', 'typescript'\n",
    "        ]\n",
    "    \n",
    "    def extract_text_from_pdf(self, pdf_path):\n",
    "        text = \"\"\n",
    "        try:\n",
    "            with open(pdf_path, 'rb') as file:\n",
    "                reader = PyPDF2.PdfReader(file)\n",
    "                for page in reader.pages:\n",
    "                    text += page.extract_text()\n",
    "            return text\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading PDF: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def extract_text_from_docx(self, docx_path):\n",
    "        try:\n",
    "            doc = docx.Document(docx_path)\n",
    "            text = \"\"\n",
    "            for paragraph in doc.paragraphs:\n",
    "                text += paragraph.text + \"\\n\"\n",
    "            return text\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading DOCX: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def parse_resume(self, resume_path):\n",
    "        print(f\"üìÑ Parsing resume: {resume_path}\")\n",
    "        \n",
    "        if resume_path.endswith('.pdf'):\n",
    "            text = self.extract_text_from_pdf(resume_path)\n",
    "        elif resume_path.endswith('.docx'):\n",
    "            text = self.extract_text_from_docx(resume_path)\n",
    "        else:\n",
    "            try:\n",
    "                with open(resume_path, 'r', encoding='utf-8') as file:\n",
    "                    text = file.read()\n",
    "            except:\n",
    "                text = resume_path \n",
    "        \n",
    "        if not text.strip():\n",
    "            return {\"error\": \"Could not extract text from resume\"}\n",
    "       \n",
    "        prompt = f\"\"\"\n",
    "        Extract the following information from this resume text. Return ONLY a JSON object with these exact keys:\n",
    "        {{\n",
    "            \"name\": \"full name\",\n",
    "            \"email\": \"email address\", \n",
    "            \"phone\": \"phone number if available\",\n",
    "            \"summary\": \"brief professional summary\",\n",
    "            \"skills\": [\"list\", \"of\", \"technical\", \"skills\"],\n",
    "            \"experience\": \"years of experience\",\n",
    "            \"education\": \"highest education level\",\n",
    "            \"current_role\": \"current/most recent job title\",\n",
    "            \"industries\": [\"list\", \"of\", \"industries\", \"worked\", \"in\"]\n",
    "        }}\n",
    "        \n",
    "        RESUME TEXT:\n",
    "        {text[:3000]}\n",
    "        \n",
    "        Return ONLY the JSON object, nothing else.\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = llm.invoke(prompt)\n",
    "            response = response.strip()\n",
    "            if response.startswith('```json'):\n",
    "                response = response[7:]\n",
    "            if response.endswith('```'):\n",
    "                response = response[:-3]\n",
    "            \n",
    "            resume_data = json.loads(response)\n",
    "            resume_data['raw_text'] = text\n",
    "            return resume_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing with Ollama: {e}\")\n",
    "            return self.simple_parse(text)\n",
    "    \n",
    "    def simple_parse(self, text):\n",
    "        skills_found = []\n",
    "        for skill in self.skills_keywords:\n",
    "            if re.search(r'\\b' + re.escape(skill) + r'\\b', text.lower()):\n",
    "                skills_found.append(skill)\n",
    "  \n",
    "        email_match = re.search(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', text)\n",
    "        email = email_match.group() if email_match else \"Not found\"\n",
    "        \n",
    "        lines = text.split('\\n')\n",
    "        name = lines[0].strip() if lines else \"Not found\"\n",
    "        \n",
    "        return {\n",
    "            'name': name,\n",
    "            'email': email,\n",
    "            'skills': skills_found,\n",
    "            'summary': \"Extracted from resume\",\n",
    "            'raw_text': text\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad81e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JobScraper:\n",
    "    def __init__(self):\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "    \n",
    "    def scrape_indeed(self, job_title, location=\"\", limit=10):\n",
    "        jobs = []\n",
    "        try:\n",
    "            base_url = \"https://www.indeed.com/jobs\"\n",
    "            params = {\n",
    "                'q': job_title,\n",
    "                'l': location,\n",
    "                'limit': limit\n",
    "            }\n",
    "            \n",
    "            response = requests.get(base_url, params=params, headers=self.headers, timeout=10)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            job_cards = soup.find_all('div', class_='job_seen_beacon')\n",
    "            \n",
    "            for card in job_cards[:limit]:\n",
    "                try:\n",
    "                    title_elem = card.find('h2', class_='jobTitle')\n",
    "                    company_elem = card.find('span', class_='companyName')\n",
    "                    location_elem = card.find('div', class_='companyLocation')\n",
    "                    link_elem = card.find('a', class_='jcs-JobTitle')\n",
    "                    \n",
    "                    if title_elem and company_elem:\n",
    "                        job = {\n",
    "                            'title': title_elem.text.strip(),\n",
    "                            'company': company_elem.text.strip(),\n",
    "                            'location': location_elem.text.strip() if location_elem else \"Not specified\",\n",
    "                            'link': \"https://www.indeed.com\" + link_elem['href'] if link_elem else \"\",\n",
    "                            'source': 'Indeed',\n",
    "                            'description': self.get_job_description(\"https://www.indeed.com\" + link_elem['href']) if link_elem else \"\"\n",
    "                        }\n",
    "                        jobs.append(job)\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping Indeed: {e}\")\n",
    "            \n",
    "        return jobs\n",
    "    \n",
    "    def scrape_linkedin_simplified(self, job_title, location=\"\", limit=10):\n",
    "        jobs = []\n",
    "        try:\n",
    "            base_url = \"https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search\"\n",
    "            params = {\n",
    "                'keywords': job_title,\n",
    "                'location': location,\n",
    "                'start': 0\n",
    "            }\n",
    "            \n",
    "            response = requests.get(base_url, params=params, headers=self.headers, timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                \n",
    "                job_cards = soup.find_all('li')\n",
    "                \n",
    "                for card in job_cards[:limit]:\n",
    "                    try:\n",
    "                        title_elem = card.find('h3', class_='base-search-card__title')\n",
    "                        company_elem = card.find('h4', class_='base-search-card__subtitle')\n",
    "                        location_elem = card.find('span', class_='job-search-card__location')\n",
    "                        link_elem = card.find('a', class_='base-card__full-link')\n",
    "                        \n",
    "                        if title_elem and company_elem:\n",
    "                            job = {\n",
    "                                'title': title_elem.text.strip(),\n",
    "                                'company': company_elem.text.strip(),\n",
    "                                'location': location_elem.text.strip() if location_elem else \"Not specified\",\n",
    "                                'link': link_elem['href'] if link_elem else \"\",\n",
    "                                'source': 'LinkedIn',\n",
    "                                'description': \"\"\n",
    "                            }\n",
    "                            jobs.append(job)\n",
    "                    except Exception as e:\n",
    "                        continue\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping LinkedIn: {e}\")\n",
    "            \n",
    "        return jobs\n",
    "    \n",
    "    def get_job_description(self, job_url):\n",
    "        try:\n",
    "            response = requests.get(job_url, headers=self.headers, timeout=10)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            description_selectors = [\n",
    "                'div#jobDescriptionText',\n",
    "                'div.job-description',\n",
    "                'div.description',\n",
    "                'div.job-details'\n",
    "            ]\n",
    "            \n",
    "            for selector in description_selectors:\n",
    "                desc_elem = soup.select_one(selector)\n",
    "                if desc_elem:\n",
    "                    return desc_elem.get_text(strip=True)[:500]\n",
    "            \n",
    "            return \"Description not available\"\n",
    "        except:\n",
    "            return \"Description not available\"\n",
    "    \n",
    "    def search_multiple_sources(self, job_title, location=\"\", jobs_per_source=5):\n",
    "        all_jobs = []\n",
    "        \n",
    "        print(\"üîç Searching Indeed...\")\n",
    "        indeed_jobs = self.scrape_indeed(job_title, location, jobs_per_source)\n",
    "        all_jobs.extend(indeed_jobs)\n",
    "        \n",
    "        print(\"üîç Searching LinkedIn...\")\n",
    "        linkedin_jobs = self.scrape_linkedin_simplified(job_title, location, jobs_per_source)\n",
    "        all_jobs.extend(linkedin_jobs)\n",
    "        \n",
    "        return all_jobs[:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4689144",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JobMatcher:\n",
    "    def __init__(self):\n",
    "        self.llm = llm\n",
    "    \n",
    "    def calculate_similarity(self, resume_text, job_description):\n",
    "        try:\n",
    "            vectorizer = TfidfVectorizer()\n",
    "            tfidf_matrix = vectorizer.fit_transform([resume_text, job_description])\n",
    "            similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
    "            return similarity[0][0]\n",
    "        except:\n",
    "            return 0\n",
    "    \n",
    "    def analyze_job_fit(self, resume_data, job):\n",
    "        prompt = f\"\"\"\n",
    "        Analyze the match between a candidate and a job opportunity. Return ONLY a JSON response with this structure:\n",
    "        {{\n",
    "            \"relevance_score\": 0-100,\n",
    "            \"skills_match\": [\"list\", \"of\", \"matching\", \"skills\"],\n",
    "            \"missing_skills\": [\"list\", \"of\", \"missing\", \"important\", \"skills\"],\n",
    "            \"fit_analysis\": \"brief analysis of why this is a good fit\",\n",
    "            \"recommendation\": \"strong_recommend|recommend|neutral|not_recommend\"\n",
    "        }}\n",
    "        \n",
    "        CANDIDATE PROFILE:\n",
    "        Name: {resume_data.get('name', 'N/A')}\n",
    "        Skills: {', '.join(resume_data.get('skills', []))}\n",
    "        Experience: {resume_data.get('experience', 'N/A')}\n",
    "        Summary: {resume_data.get('summary', 'N/A')}\n",
    "        \n",
    "        JOB OPPORTUNITY:\n",
    "        Title: {job['title']}\n",
    "        Company: {job['company']}\n",
    "        Description: {job['description'][:1000]}\n",
    "        \n",
    "        Analyze the skills match, experience relevance, and overall fit.\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.llm.invoke(prompt)\n",
    "            response = response.strip()\n",
    "            if response.startswith('```json'):\n",
    "                response = response[7:]\n",
    "            if response.endswith('```'):\n",
    "                response = response[:-3]\n",
    "            \n",
    "            analysis = json.loads(response)\n",
    "            \n",
    "            analysis['text_similarity'] = self.calculate_similarity(\n",
    "                resume_data.get('raw_text', ''),\n",
    "                job['description']\n",
    "            )\n",
    "            \n",
    "            return analysis\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in job fit analysis: {e}\")\n",
    "            return {\n",
    "                \"relevance_score\": 50,\n",
    "                \"skills_match\": [],\n",
    "                \"missing_skills\": [],\n",
    "                \"fit_analysis\": \"Analysis unavailable\",\n",
    "                \"recommendation\": \"neutral\",\n",
    "                \"text_similarity\": 0\n",
    "            }\n",
    "    \n",
    "    def rank_jobs(self, resume_data, jobs):\n",
    "        ranked_jobs = []\n",
    "        \n",
    "        print(\"üéØ Analyzing job matches...\")\n",
    "        for i, job in enumerate(jobs):\n",
    "            print(f\"  Analyzing job {i+1}/{len(jobs)}...\")\n",
    "            \n",
    "            analysis = self.analyze_job_fit(resume_data, job)\n",
    "            \n",
    "            overall_score = (analysis['relevance_score'] * 0.7 + \n",
    "                           analysis['text_similarity'] * 100 * 0.3)\n",
    "            \n",
    "            ranked_jobs.append({\n",
    "                **job,\n",
    "                'analysis': analysis,\n",
    "                'overall_score': overall_score\n",
    "            })\n",
    "            \n",
    "          \n",
    "            time.sleep(1)\n",
    "        \n",
    "      \n",
    "        ranked_jobs.sort(key=lambda x: x['overall_score'], reverse=True)\n",
    "        return ranked_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3d36fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_relevant_jobs(resume_path, job_title, location=\"\", num_jobs=10):\n",
    "    \"\"\"\n",
    "    Main function to find relevant jobs based on resume\n",
    "    \"\"\"\n",
    "    print(\"üöÄ Starting Job Search Agent...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "\n",
    "    parser = ResumeParser()\n",
    "    resume_data = parser.parse_resume(resume_path)\n",
    "    \n",
    "    if 'error' in resume_data:\n",
    "        print(f\"‚ùå Error: {resume_data['error']}\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"‚úÖ Resume parsed for: {resume_data.get('name', 'Unknown')}\")\n",
    "    print(f\"üìß Email: {resume_data.get('email', 'Not found')}\")\n",
    "    print(f\"üõ†Ô∏è Skills: {', '.join(resume_data.get('skills', []))}\")\n",
    "    print()\n",
    "   \n",
    "    scraper = JobScraper()\n",
    "    jobs = scraper.search_multiple_sources(job_title, location, num_jobs//2)\n",
    "    \n",
    "    if not jobs:\n",
    "        print(\"‚ùå No jobs found. Try different search terms.\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"‚úÖ Found {len(jobs)} jobs to analyze\")\n",
    "    print()\n",
    "    matcher = JobMatcher()\n",
    "    ranked_jobs = matcher.rank_jobs(resume_data, jobs)\n",
    "    \n",
    "    return ranked_jobs, resume_data\n",
    "\n",
    "def display_results(ranked_jobs, resume_data, top_k=10):\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"üéØ TOP JOB RECOMMENDATIONS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for i, job in enumerate(ranked_jobs[:top_k]):\n",
    "        print(f\"\\nüèÜ #{i+1} | Score: {job['overall_score']:.1f}/100\")\n",
    "        print(f\"üìå Title: {job['title']}\")\n",
    "        print(f\"üè¢ Company: {job['company']}\")\n",
    "        print(f\"üìç Location: {job['location']}\")\n",
    "        print(f\"üîó Source: {job['source']}\")\n",
    "        print(f\"üîó Link: {job['link']}\")\n",
    "        \n",
    "        analysis = job['analysis']\n",
    "        print(f\"‚úÖ Matching Skills: {', '.join(analysis['skills_match'][:5])}\")\n",
    "        if analysis['missing_skills']:\n",
    "            print(f\"‚ö†Ô∏è Missing: {', '.join(analysis['missing_skills'][:3])}\")\n",
    "        print(f\"üìä Analysis: {analysis['fit_analysis']}\")\n",
    "        print(f\"üí° Recommendation: {analysis['recommendation'].replace('_', ' ').title()}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "def save_results_to_csv(ranked_jobs, filename=\"job_recommendations.csv\"):\n",
    "    data = []\n",
    "    for job in ranked_jobs:\n",
    "        data.append({\n",
    "            'Title': job['title'],\n",
    "            'Company': job['company'],\n",
    "            'Location': job['location'],\n",
    "            'Score': f\"{job['overall_score']:.1f}\",\n",
    "            'Link': job['link'],\n",
    "            'Source': job['source'],\n",
    "            'Matching Skills': ', '.join(job['analysis']['skills_match'][:5]),\n",
    "            'Missing Skills': ', '.join(job['analysis']['missing_skills'][:3]),\n",
    "            'Analysis': job['analysis']['fit_analysis']\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"üíæ Results saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49610162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing Job Search System...\n",
      "üöÄ Starting Job Search Agent...\n",
      "==================================================\n",
      "üìÑ Parsing resume: sample_resume.txt\n",
      "Error parsing with Ollama: Unterminated string starting at: line 43 column 18 (char 519)\n",
      "‚úÖ Resume parsed for: \n",
      "üìß Email: rhea.1@email.com\n",
      "üõ†Ô∏è Skills: python, sql, machine learning, deep learning, tensorflow, pytorch, aws, docker, kubernetes, git, data analysis, ml, nlp, cloud\n",
      "\n",
      "üîç Searching Indeed...\n",
      "üîç Searching LinkedIn...\n",
      "‚úÖ Found 5 jobs to analyze\n",
      "\n",
      "üéØ Analyzing job matches...\n",
      "  Analyzing job 1/5...\n",
      "  Analyzing job 2/5...\n",
      "  Analyzing job 3/5...\n",
      "  Analyzing job 4/5...\n",
      "  Analyzing job 5/5...\n",
      "\n",
      "================================================================================\n",
      "üéØ TOP JOB RECOMMENDATIONS\n",
      "================================================================================\n",
      "\n",
      "üèÜ #1 | Score: 59.5/100\n",
      "üìå Title: Data Scientist- Across PAN India\n",
      "üè¢ Company: Capgemini Engineering\n",
      "üìç Location: Bengaluru, Karnataka, India\n",
      "üîó Source: LinkedIn\n",
      "üîó Link: https://in.linkedin.com/jobs/view/data-scientist-across-pan-india-at-capgemini-engineering-4296207682?position=1&pageNum=0&refId=C82IrQTox%2BW7TiOJ8%2BITIQ%3D%3D&trackingId=vhu8TAxDAPxrFUxA7%2BlWUA%3D%3D\n",
      "‚úÖ Matching Skills: python, sql, machine learning, deep learning, tensorflow\n",
      "‚ö†Ô∏è Missing: experience in data science field, proven track record with large datasets, knowledge of specific industry applications relevant to Capgemini Engineering's projects\n",
      "üìä Analysis: The candidate has a strong technical background and skills set that align well with the job description. However, lacking direct experience may require additional training or mentorship.\n",
      "üí° Recommendation: Strong Recommend\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üèÜ #2 | Score: 52.5/100\n",
      "üìå Title: AI / ML Engineer\n",
      "üè¢ Company: HDFC Bank\n",
      "üìç Location: Bengaluru, Karnataka, India\n",
      "üîó Source: LinkedIn\n",
      "üîó Link: https://in.linkedin.com/jobs/view/ai-ml-engineer-at-hdfc-bank-4302326597?position=2&pageNum=0&refId=C82IrQTox%2BW7TiOJ8%2BITIQ%3D%3D&trackingId=ZYXMki9Opjv2Y%2BhyaaK3%2Fw%3D%3D\n",
      "‚úÖ Matching Skills: python, sql, machine learning, deep learning, tensorflow\n",
      "‚ö†Ô∏è Missing: experience in finance or banking industry, knowledge of HDFC Bank's specific systems and processes\n",
      "üìä Analysis: The candidate has a strong technical background with skills relevant to AI/ML engineering. However, the lack of experience within the financial sector may be a concern for this role at HDFC Bank.\n",
      "üí° Recommendation: Neutral\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üèÜ #3 | Score: 52.5/100\n",
      "üìå Title: Staff AI Scientist\n",
      "üè¢ Company: Intuit\n",
      "üìç Location: Bengaluru, Karnataka, India\n",
      "üîó Source: LinkedIn\n",
      "üîó Link: https://in.linkedin.com/jobs/view/staff-ai-scientist-at-intuit-4303899259?position=3&pageNum=0&refId=C82IrQTox%2BW7TiOJ8%2BITIQ%3D%3D&trackingId=c411lwW%2BIdotC%2Bi3dzD3sQ%3D%3D\n",
      "‚úÖ Matching Skills: python, machine learning, deep learning, aws, git\n",
      "‚ö†Ô∏è Missing: experience in AI field, specific knowledge of Intuit's products and services\n",
      "üìä Analysis: The candidate has a strong foundation in technical skills relevant to the job, including Python programming languages used for machine learning tasks. Their experience with cloud platforms like AWS aligns well with industry standards.\n",
      "üí° Recommendation: Strong Recommend\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üèÜ #4 | Score: 52.5/100\n",
      "üìå Title: Data Scientist\n",
      "üè¢ Company: Microsoft\n",
      "üìç Location: Bengaluru, Karnataka, India\n",
      "üîó Source: LinkedIn\n",
      "üîó Link: https://in.linkedin.com/jobs/view/data-scientist-at-microsoft-4311367673?position=4&pageNum=0&refId=C82IrQTox%2BW7TiOJ8%2BITIQ%3D%3D&trackingId=aWISfSKXGhCVVSKoomYTiQ%3D%3D\n",
      "‚úÖ Matching Skills: python, sql, machine learning, deep learning, aws\n",
      "‚ö†Ô∏è Missing: experience in data science field, proven track record of successful projects or research work related to the job description\n",
      "üìä Analysis: The candidate has a strong set of technical skills relevant for a Data Scientist role, including machine learning and cloud services. However, without experience specifically mentioned as 'data scientist' in their profile.\n",
      "üí° Recommendation: Strong Recommend\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üèÜ #5 | Score: 52.5/100\n",
      "üìå Title: Data Scientist\n",
      "üè¢ Company: AB InBev GCC India\n",
      "üìç Location: Bengaluru, Karnataka, India\n",
      "üîó Source: LinkedIn\n",
      "üîó Link: https://in.linkedin.com/jobs/view/data-scientist-at-ab-inbev-gcc-india-4304090614?position=5&pageNum=0&refId=C82IrQTox%2BW7TiOJ8%2BITIQ%3D%3D&trackingId=0qY7tZupE28sfD0ZBhF8Cw%3D%3D\n",
      "‚úÖ Matching Skills: python, sql, machine learning, deep learning, tensorflow\n",
      "‚ö†Ô∏è Missing: experience in the beverage industry, knowledge of AB InBev's specific data science needs and practices\n",
      "üìä Analysis: The candidate has a strong technical background with skills relevant to machine learning which is often required for Data Scientist roles. However, lacking direct experience within the target company or its sector may affect their fit.\n",
      "üí° Recommendation: Neutral\n",
      "--------------------------------------------------------------------------------\n",
      "üíæ Results saved to job_recommendations.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def test_system():\n",
    "\n",
    "    sample_resume_text = \"\"\"\n",
    "    Rhea\n",
    "    Senior Data Scientist\n",
    "    rhea.1@email.com | 1122334455 | linkedin.com/in/rhea\n",
    "    \n",
    "    SUMMARY:\n",
    "    Experienced Data Scientist with 5+ years in machine learning, NLP, and cloud technologies. \n",
    "    Strong background in Python, TensorFlow, and AWS.\n",
    "    \n",
    "    EXPERIENCE:\n",
    "    Senior Data Scientist - TechCorp (2020-Present)\n",
    "    - Led ML projects improving customer recommendations by 30%\n",
    "    - Developed NLP models for sentiment analysis\n",
    "    - Managed AWS infrastructure for ML pipelines\n",
    "    \n",
    "    Data Scientist - DataWorks (2018-2020)\n",
    "    - Built predictive models using Python and Scikit-learn\n",
    "    - Implemented deep learning solutions with TensorFlow\n",
    "    \n",
    "    SKILLS:\n",
    "    Python, Machine Learning, Deep Learning, TensorFlow, PyTorch, NLP, \n",
    "    AWS, SQL, Docker, Kubernetes, Git, Data Analysis\n",
    "    \n",
    "    EDUCATION:\n",
    "    MS Computer Science - IIT\n",
    "    BS Mathematics - VIT\n",
    "    \"\"\"\n",
    "\n",
    "    with open(\"sample_resume.txt\", \"w\") as f:\n",
    "        f.write(sample_resume_text)\n",
    "  \n",
    "    job_title = \"data scientist\"\n",
    "    location = \"Bangalore\"\n",
    "    \n",
    "    ranked_jobs, resume_data = find_relevant_jobs(\n",
    "        \"sample_resume.txt\", \n",
    "        job_title, \n",
    "        location\n",
    "    )\n",
    "    \n",
    "    if ranked_jobs:\n",
    "        display_results(ranked_jobs, resume_data)\n",
    "        save_results_to_csv(ranked_jobs)\n",
    "    else:\n",
    "        print(\"‚ùå No relevant jobs found.\")\n",
    "print(\"üß™ Testing Job Search System...\")\n",
    "test_system()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39ab7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Starting job search with your resume...\n",
      "üöÄ Starting Job Search Agent...\n",
      "==================================================\n",
      "üìÑ Parsing resume: /Users/siddharthshailendra/Resume_Shortlister_using_llm_basic/sample_resume_2.txt\n",
      "Error parsing with Ollama: Expecting value: line 41 column 17 (char 520)\n",
      "‚úÖ Resume parsed for: Arjun Sharma\n",
      "üìß Email: arjun.sharma@email.com\n",
      "üõ†Ô∏è Skills: python, sql, machine learning, deep learning, tensorflow, pytorch, aws, docker, kubernetes, git, ci/cd, artificial intelligence, ai, ml, nlp, computer vision, spark, kafka, gcp, cloud, mobile\n",
      "\n",
      "üîç Searching Indeed...\n",
      "üîç Searching LinkedIn...\n",
      "‚úÖ Found 5 jobs to analyze\n",
      "\n",
      "üéØ Analyzing job matches...\n",
      "  Analyzing job 1/5...\n",
      "Error in job fit analysis: Unterminated string starting at: line 13 column 5 (char 717)\n",
      "  Analyzing job 2/5...\n",
      "  Analyzing job 3/5...\n",
      "  Analyzing job 4/5...\n",
      "  Analyzing job 5/5...\n",
      "\n",
      "================================================================================\n",
      "üéØ TOP JOB RECOMMENDATIONS\n",
      "================================================================================\n",
      "\n",
      "üèÜ #1 | Score: 59.5/100\n",
      "üìå Title: Data scientist- Python- AI/ML GEN AI- Across india\n",
      "üè¢ Company: Capgemini Engineering\n",
      "üìç Location: Mumbai, Maharashtra, India\n",
      "üîó Source: LinkedIn\n",
      "üîó Link: https://in.linkedin.com/jobs/view/data-scientist-python-ai-ml-gen-ai-across-india-at-capgemini-engineering-4295891624?position=3&pageNum=0&refId=9ogJbiF7OtKPPrXN6mMidw%3D%3D&trackingId=iPqgf0aR7Nn2WQM%2Ba6%2FKlA%3D%3D\n",
      "‚úÖ Matching Skills: python, machine learning, deep learning, tensorflow, pytorch\n",
      "‚ö†Ô∏è Missing: experience in data science field, knowledge of spark and kafka for big data processing, understanding of gcp services beyond basic knowledge\n",
      "üìä Analysis: Arjun Sharma has a strong technical background with skills relevant to the job description. His expertise aligns well with Capgemini Engineering's focus on Python, AI/ML and cloud technologies.\n",
      "üí° Recommendation: Strong Recommend\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üèÜ #2 | Score: 52.5/100\n",
      "üìå Title: Data Scientist\n",
      "üè¢ Company: Deloitte\n",
      "üìç Location: Mumbai, Maharashtra, India\n",
      "üîó Source: LinkedIn\n",
      "üîó Link: https://in.linkedin.com/jobs/view/data-scientist-at-deloitte-4307550628?position=2&pageNum=0&refId=9ogJbiF7OtKPPrXN6mMidw%3D%3D&trackingId=bgZBL%2Blt8Ddiiq%2Bi7TJizA%3D%3D\n",
      "‚úÖ Matching Skills: python, sql, machine learning, deep learning, tensorflow\n",
      "‚ö†Ô∏è Missing: experience in data science field, proven track record with big datasets, knowledge of statistical analysis methods\n",
      "üìä Analysis: Arjun Sharma has a strong technical skill set that aligns well with the job requirements for Deloitte's Data Scientist position. His expertise in machine learning, deep learning frameworks like TensorFlow and PyTorch, as well as cloud services such as AWS are highly relevant to data science roles.\n",
      "üí° Recommendation: Strong Recommend\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üèÜ #3 | Score: 52.5/100\n",
      "üìå Title: Data Scientist\n",
      "üè¢ Company: Procter & Gamble\n",
      "üìç Location: Mumbai, Maharashtra, India\n",
      "üîó Source: LinkedIn\n",
      "üîó Link: https://in.linkedin.com/jobs/view/data-scientist-at-procter-gamble-4303485542?position=4&pageNum=0&refId=9ogJbiF7OtKPPrXN6mMidw%3D%3D&trackingId=DIpGi9X8f4t6nPnOKBdpSQ%3D%3D\n",
      "‚úÖ Matching Skills: python, sql, machine learning, deep learning, tensorflow\n",
      "‚ö†Ô∏è Missing: experience in data science field, proven track record with large datasets and complex analytics projects, familiarity with specific industry applications relevant to Procter & Gamble's products or services\n",
      "üìä Analysis: Arjun Sharma has a strong technical background that aligns well with the data science role at P&G. His skills in machine learning, deep learning, and AI are particularly valuable for innovation within this industry.\n",
      "üí° Recommendation: Strong Recommend\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üèÜ #4 | Score: 52.5/100\n",
      "üìå Title: AI/ML‚Äì Applied Artificial Intelligence & Machine Learning Internship in Mumbai (Hybrid)\n",
      "üè¢ Company: Reflex Realty LLP\n",
      "üìç Location: Mumbai Metropolitan Region\n",
      "üîó Source: LinkedIn\n",
      "üîó Link: https://in.linkedin.com/jobs/view/ai-ml%E2%80%93-applied-artificial-intelligence-machine-learning-internship-in-mumbai-hybrid-at-reflex-realty-llp-4295853106?position=5&pageNum=0&refId=9ogJbiF7OtKPPrXN6mMidw%3D%3D&trackingId=v8jZfIqNtY%2FXOC%2F%2FyuaqnQ%3D%3D\n",
      "‚úÖ Matching Skills: python, machine learning, artificial intelligence\n",
      "‚ö†Ô∏è Missing: experience in AI/ML field, specific knowledge of cloud services like aws and gcp beyond general awareness\n",
      "üìä Analysis: Arjun Sharma has a strong foundation in programming languages, machine learning frameworks (tensorflow, pytorch), as well as some experience with containerization technologies (docker) which are relevant to the hybrid work environment of Reflex Realty LLP. However, his lack of direct AI/ML industry experience might be a concern for this internship role.\n",
      "üí° Recommendation: Neutral\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üèÜ #5 | Score: 35.0/100\n",
      "üìå Title: Data Scientist\n",
      "üè¢ Company: Deloitte\n",
      "üìç Location: Mumbai, Maharashtra, India\n",
      "üîó Source: LinkedIn\n",
      "üîó Link: https://in.linkedin.com/jobs/view/data-scientist-at-deloitte-4293182219?position=1&pageNum=0&refId=9ogJbiF7OtKPPrXN6mMidw%3D%3D&trackingId=k6TUpQEPKxW2g7%2Fq5f2C9w%3D%3D\n",
      "‚úÖ Matching Skills: \n",
      "üìä Analysis: Analysis unavailable\n",
      "üí° Recommendation: Neutral\n",
      "--------------------------------------------------------------------------------\n",
      "üíæ Results saved to my_job_recommendations.csv\n",
      "\n",
      "üéâ Found 5 relevant jobs!\n",
      "üí° Tips:\n",
      "  - Apply to top 3-5 jobs first\n",
      "  - Customize your resume for each application\n",
      "  - Use the analysis to improve your resume\n"
     ]
    }
   ],
   "source": [
    "def search_jobs_with_your_resume():\n",
    "    YOUR_RESUME_PATH = \"/Users/siddharthshailendra/Resume_Shortlister_using_llm_basic/sample_resume_2.txt\"  # Remove extra slash\n",
    "    JOB_TITLE = \"machine learning engineer\"  \n",
    "    LOCATION = \"Mumbai\" \n",
    "    \n",
    "    print(\"üîç Starting job search with your resume...\")\n",
    "    ranked_jobs, resume_data = find_relevant_jobs(\n",
    "        YOUR_RESUME_PATH,\n",
    "        JOB_TITLE, \n",
    "        LOCATION\n",
    "    )\n",
    "    \n",
    "    if ranked_jobs:\n",
    "        display_results(ranked_jobs, resume_data)\n",
    "        save_results_to_csv(ranked_jobs, \"my_job_recommendations.csv\")\n",
    "        \n",
    "        print(f\"\\nüéâ Found {len(ranked_jobs)} relevant jobs!\")\n",
    "        print(\"üí° Tips:\")\n",
    "        print(\"  - Apply to top 3-5 jobs first\")\n",
    "        print(\"  - Customize your resume for each application\")\n",
    "        print(\"  - Use the analysis to improve your resume\")\n",
    "    else:\n",
    "        print(\"‚ùå No relevant jobs found. Try:\")\n",
    "        print(\"  - Different job titles\")\n",
    "        print(\"  - Broader location search\")\n",
    "        print(\"  - Check your resume format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43f2cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ ENHANCED JOB SEARCH STARTING...\n",
      "============================================================\n",
      "üöÄ Starting Job Search Agent...\n",
      "==================================================\n",
      "üìÑ Parsing resume: /Users/siddharthshailendra/Resume_Shortlister_using_llm_basic/sample_resume_2.txt\n",
      "Error parsing with Ollama: Expecting value: line 41 column 17 (char 520)\n",
      "‚úÖ Resume parsed for: Arjun Sharma\n",
      "üìß Email: arjun.sharma@email.com\n",
      "üõ†Ô∏è Skills: python, sql, machine learning, deep learning, tensorflow, pytorch, aws, docker, kubernetes, git, ci/cd, artificial intelligence, ai, ml, nlp, computer vision, spark, kafka, gcp, cloud, mobile\n",
      "\n",
      "üîç Searching Indeed...\n",
      "üîç Searching LinkedIn...\n",
      "‚úÖ Found 5 jobs to analyze\n",
      "\n",
      "üéØ Analyzing job matches...\n",
      "  Analyzing job 1/5...\n",
      "  Analyzing job 2/5...\n",
      "  Analyzing job 3/5...\n",
      "  Analyzing job 4/5...\n",
      "  Analyzing job 5/5...\n",
      "‚úÖ Jobs analyzed successfully!\n",
      "üíæ Results saved to my_job_recommendations.csv\n",
      "üéØ JOB SEARCH RESULTS SUMMARY\n",
      "======================================================================\n",
      "üìä Total Jobs Found: 5\n",
      "üèÜ Average Score: 53.9/100\n",
      "üíº Top Companies: Capgemini Engineering, Deloitte, Deloitte\n",
      "\n",
      "üèÖ RANK #1 | SCORE: 59.5/100\n",
      "üìå Data scientist- Python- AI/ML GEN AI- Across india\n",
      "üè¢ Capgemini Engineering | üìç Mumbai, Maharashtra, India\n",
      "üîó https://in.linkedin.com/jobs/view/data-scientist-python-ai-ml-gen-ai-across-india-at-capgemini-engineering-4295891624?position=3&pageNum=0&refId=swnV5erkbv6Hq8y5pQ%2BnJw%3D%3D&trackingId=ALJXuG2zruaBKVvy24tYpA%3D%3D\n",
      "üì± Source: LinkedIn\n",
      "‚úÖ Matching Skills: python, machine learning, deep learning, tensorflow, pytorch\n",
      "‚ö†Ô∏è  Missing Skills: experience in data science field, knowledge of spark and kafka, understanding of\n",
      "gcp services\n",
      "üìã Analysis: Arjun Sharma has a strong foundation in Python, machine learning, deep learning, AI/ML tools like\n",
      "TensorFlow and PyTorch, as well as cloud technologies such as AWS. His skills align closely with the\n",
      "job requirements for an entry-level Data Scientist role at Capgemini Engineering.\n",
      "üí° Recommendation: Strong Recommend\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üèÖ RANK #2 | SCORE: 52.5/100\n",
      "üìå Data Scientist\n",
      "üè¢ Deloitte | üìç Mumbai, Maharashtra, India\n",
      "üîó https://in.linkedin.com/jobs/view/data-scientist-at-deloitte-4293182219?position=1&pageNum=0&refId=swnV5erkbv6Hq8y5pQ%2BnJw%3D%3D&trackingId=%2FsVMwlY74d7Ibe0P470NhA%3D%3D\n",
      "üì± Source: LinkedIn\n",
      "‚úÖ Matching Skills: python, sql, machine learning, deep learning, tensorflow\n",
      "‚ö†Ô∏è  Missing Skills: experience in data science field, proven track record with big datasets,\n",
      "knowledge of statistical analysis methods\n",
      "üìã Analysis: Arjun Sharma has a strong technical background and is proficient in several skills relevant to the\n",
      "Data Scientist role at Deloitte. However, his lack of experience directly related to data science\n",
      "may be seen as a gap.\n",
      "üí° Recommendation: Neutral\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üèÖ RANK #3 | SCORE: 52.5/100\n",
      "üìå Data Scientist\n",
      "üè¢ Deloitte | üìç Mumbai, Maharashtra, India\n",
      "üîó https://in.linkedin.com/jobs/view/data-scientist-at-deloitte-4307550628?position=2&pageNum=0&refId=swnV5erkbv6Hq8y5pQ%2BnJw%3D%3D&trackingId=RijiaT08vFBb4x239hfS2g%3D%3D\n",
      "üì± Source: LinkedIn\n",
      "‚úÖ Matching Skills: python, sql, machine learning, deep learning, tensorflow\n",
      "‚ö†Ô∏è  Missing Skills: experience in data science field, proven track record with big datasets,\n",
      "knowledge of statistical analysis methods\n",
      "üìã Analysis: Arjun Sharma has a strong technical skill set that aligns well with the job description for Data\n",
      "Scientist at Deloitte. His expertise in machine learning, deep learning frameworks like TensorFlow\n",
      "and PyTorch, as well as cloud services such as AWS are highly relevant to data science roles.\n",
      "üí° Recommendation: Strong Recommend\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üèÖ RANK #4 | SCORE: 52.5/100\n",
      "üìå Data Scientist\n",
      "üè¢ Procter & Gamble | üìç Mumbai, Maharashtra, India\n",
      "üîó https://in.linkedin.com/jobs/view/data-scientist-at-procter-gamble-4303485542?position=4&pageNum=0&refId=swnV5erkbv6Hq8y5pQ%2BnJw%3D%3D&trackingId=hfqfFetBZaj4ksJhWQqnIg%3D%3D\n",
      "üì± Source: LinkedIn\n",
      "‚úÖ Matching Skills: python, sql, machine learning, deep learning, tensorflow\n",
      "‚ö†Ô∏è  Missing Skills: experience in data science field, procter & gamble specific industry knowledge\n",
      "üìã Analysis: Arjun Sharma has a strong technical background with skills relevant to the Data Scientist role at\n",
      "Procter & Gamble. However, his lack of direct experience within the company or its sector may\n",
      "require additional on-the-job learning.\n",
      "üí° Recommendation: Neutral\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üèÖ RANK #5 | SCORE: 52.5/100\n",
      "üìå AI/ML‚Äì Applied Artificial Intelligence & Machine Learning Internship in Mumbai (Hybrid)\n",
      "üè¢ Reflex Realty LLP | üìç Mumbai Metropolitan Region\n",
      "üîó https://in.linkedin.com/jobs/view/ai-ml%E2%80%93-applied-artificial-intelligence-machine-learning-internship-in-mumbai-hybrid-at-reflex-realty-llp-4295853106?position=5&pageNum=0&refId=swnV5erkbv6Hq8y5pQ%2BnJw%3D%3D&trackingId=ZBFI%2BevGf9SthslHabUTSg%3D%3D\n",
      "üì± Source: LinkedIn\n",
      "‚úÖ Matching Skills: python, machine learning, artificial intelligence\n",
      "‚ö†Ô∏è  Missing Skills: experience in AI/ML field, specific knowledge of cloud services used by Reflex\n",
      "Realty LLP (if different from aws), understanding of the local market and\n",
      "industry specifics for Mumbai region\n",
      "üìã Analysis: Arjun Sharma has a strong foundation in machine learning, artificial intelligence, and related\n",
      "programming languages which align with the job's technical requirements. However, his lack of\n",
      "experience is noticeable.\n",
      "üí° Recommendation: Neutral\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìà JOB SEARCH INSIGHTS\n",
      "============================================================\n",
      "üéØ Average Match Score: 53.9/100\n",
      "üèÜ Highest Score: 59.5/100\n",
      "üìä Score Range: 52.5 - 59.5\n",
      "\n",
      "üìà SCORE DISTRIBUTION:\n",
      "   üéØ Excellent (80-100): 0 jobs\n",
      "   ‚úÖ Good (60-79): 0 jobs\n",
      "   ‚ö†Ô∏è  Average (40-59): 5 jobs\n",
      "   üî¥ Low (0-39): 0 jobs\n",
      "\n",
      "üè¢ TOP COMPANIES:\n",
      "   ‚Ä¢ Deloitte: 2 jobs (avg: 52.5/100)\n",
      "   ‚Ä¢ Capgemini Engineering: 1 jobs (avg: 59.5/100)\n",
      "   ‚Ä¢ Procter & Gamble: 1 jobs (avg: 52.5/100)\n",
      "\n",
      "üõ†Ô∏è  TOP MATCHING SKILLS:\n",
      "   ‚Ä¢ python: 5 jobs\n",
      "   ‚Ä¢ machine learning: 5 jobs\n",
      "   ‚Ä¢ deep learning: 4 jobs\n",
      "   ‚Ä¢ tensorflow: 4 jobs\n",
      "   ‚Ä¢ sql: 3 jobs\n",
      "\n",
      "üìö SKILLS TO IMPROVE:\n",
      "   ‚Ä¢ experience in data science field: missing in 4 jobs\n",
      "   ‚Ä¢ proven track record with big datasets: missing in 2 jobs\n",
      "   ‚Ä¢ knowledge of statistical analysis methods: missing in 2 jobs\n",
      "\n",
      "üí° ACTION PLAN:\n",
      "   üéØ Apply to top 3 jobs based on score\n",
      "   üìö Focus on learning: experience in data science field\n",
      "   ‚è∞ Apply within 48 hours for best results\n",
      "\n",
      "üéâ SEARCH COMPLETED! Found 5 relevant jobs!\n",
      "\n",
      "üíº NEXT STEPS:\n",
      "   1. Apply to top 3 jobs today\n",
      "   2. Customize cover letters for each application\n",
      "   3. Follow up in 5-7 days\n",
      "   4. Track applications in a spreadsheet\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, Markdown\n",
    "import textwrap\n",
    "\n",
    "def display_csv_results_clean(csv_file_path=\"my_job_recommendations.csv\"):\n",
    "    \"\"\"\n",
    "    Clean display of CSV results without truncation\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "        \n",
    "        print(\"üéØ JOB SEARCH RESULTS SUMMARY\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"üìä Total Jobs Found: {len(df)}\")\n",
    "        print(f\"üèÜ Average Score: {df['Score'].mean():.1f}/100\")\n",
    "        print(f\"üíº Top Companies: {', '.join(df['Company'].head(3).tolist())}\")\n",
    "        print()\n",
    "    \n",
    "        for idx, row in df.iterrows():\n",
    "            print(f\"üèÖ RANK #{idx+1} | SCORE: {row['Score']}/100\")\n",
    "            print(f\"üìå {row['Title']}\")\n",
    "            print(f\"üè¢ {row['Company']} | üìç {row['Location']}\")\n",
    "            print(f\"üîó {row['Link']}\")\n",
    "            print(f\"üì± Source: {row['Source']}\")\n",
    "            \n",
    "            matching_skills = str(row['Matching Skills'])\n",
    "            if len(matching_skills) > 80:\n",
    "                matching_skills = textwrap.fill(matching_skills, width=80)\n",
    "            \n",
    "            print(f\"‚úÖ Matching Skills: {matching_skills}\")\n",
    "            \n",
    "            missing_skills = str(row['Missing Skills'])\n",
    "            if missing_skills != 'nan' and missing_skills.strip():\n",
    "                if len(missing_skills) > 80:\n",
    "                    missing_skills = textwrap.fill(missing_skills, width=80)\n",
    "                print(f\"‚ö†Ô∏è  Missing Skills: {missing_skills}\")\n",
    "        \n",
    "            analysis = str(row['Analysis'])\n",
    "            if len(analysis) > 100:\n",
    "                analysis = textwrap.fill(analysis, width=100)\n",
    "            print(f\"üìã Analysis: {analysis}\")\n",
    "            \n",
    "            print(f\"üí° Recommendation: {row['Recommendation']}\")\n",
    "            print(\"‚îÄ\" * 70)\n",
    "            print()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error displaying results: {e}\")\n",
    "\n",
    "def display_job_search_insights(csv_file_path=\"my_job_recommendations.csv\"):\n",
    "    \"\"\"\n",
    "    Show insights and analytics from the job search\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "        \n",
    "        print(\"üìà JOB SEARCH INSIGHTS\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        df['Score_Numeric'] = pd.to_numeric(df['Score'], errors='coerce')\n",
    "        \n",
    "        print(f\"üéØ Average Match Score: {df['Score_Numeric'].mean():.1f}/100\")\n",
    "        print(f\"üèÜ Highest Score: {df['Score_Numeric'].max():.1f}/100\")\n",
    "        print(f\"üìä Score Range: {df['Score_Numeric'].min():.1f} - {df['Score_Numeric'].max():.1f}\")\n",
    "        print()\n",
    "        \n",
    "        excellent = len(df[df['Score_Numeric'] >= 80])\n",
    "        good = len(df[(df['Score_Numeric'] >= 60) & (df['Score_Numeric'] < 80)])\n",
    "        average = len(df[(df['Score_Numeric'] >= 40) & (df['Score_Numeric'] < 60)])\n",
    "        low = len(df[df['Score_Numeric'] < 40])\n",
    "        \n",
    "        print(\"üìà SCORE DISTRIBUTION:\")\n",
    "        print(f\"   üéØ Excellent (80-100): {excellent} jobs\")\n",
    "        print(f\"   ‚úÖ Good (60-79): {good} jobs\")\n",
    "        print(f\"   ‚ö†Ô∏è  Average (40-59): {average} jobs\")\n",
    "        print(f\"   üî¥ Low (0-39): {low} jobs\")\n",
    "        print()\n",
    "        \n",
    "        print(\"üè¢ TOP COMPANIES:\")\n",
    "        company_counts = df['Company'].value_counts()\n",
    "        for company, count in company_counts.head(3).items():\n",
    "            avg_score = df[df['Company'] == company]['Score_Numeric'].mean()\n",
    "            print(f\"   ‚Ä¢ {company}: {count} jobs (avg: {avg_score:.1f}/100)\")\n",
    "        print()\n",
    "        \n",
    "        all_matching_skills = []\n",
    "        for skills in df['Matching Skills'].dropna():\n",
    "            if str(skills) != 'nan':\n",
    "                all_matching_skills.extend([s.strip() for s in str(skills).split(',')])\n",
    "        \n",
    "        if all_matching_skills:\n",
    "            from collections import Counter\n",
    "            skill_counts = Counter(all_matching_skills)\n",
    "            print(\"üõ†Ô∏è  TOP MATCHING SKILLS:\")\n",
    "            for skill, count in skill_counts.most_common(5):\n",
    "                print(f\"   ‚Ä¢ {skill}: {count} jobs\")\n",
    "    \n",
    "        all_missing_skills = []\n",
    "        for skills in df['Missing Skills'].dropna():\n",
    "            if str(skills) != 'nan' and skills.strip():\n",
    "                all_missing_skills.extend([s.strip() for s in str(skills).split(',')])\n",
    "        \n",
    "        if all_missing_skills:\n",
    "            missing_counts = Counter(all_missing_skills)\n",
    "            print(\"\\nüìö SKILLS TO IMPROVE:\")\n",
    "            for skill, count in missing_counts.most_common(3):\n",
    "                print(f\"   ‚Ä¢ {skill}: missing in {count} jobs\")\n",
    "        \n",
    "        print(\"\\nüí° ACTION PLAN:\")\n",
    "        high_score_jobs = df[df['Score_Numeric'] >= 70]\n",
    "        if len(high_score_jobs) > 0:\n",
    "            print(f\"   üéØ Apply to {len(high_score_jobs)} high-match jobs first\")\n",
    "        else:\n",
    "            print(f\"   üéØ Apply to top {min(3, len(df))} jobs based on score\")\n",
    "        \n",
    "        if all_missing_skills:\n",
    "            top_missing = missing_counts.most_common(1)[0][0]\n",
    "            print(f\"   üìö Focus on learning: {top_missing}\")\n",
    "        \n",
    "        print(f\"   ‚è∞ Apply within 48 hours for best results\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error generating insights: {e}\")\n",
    "\n",
    "def run_enhanced_job_search():\n",
    "    \n",
    "    YOUR_RESUME_PATH = \"/Users/siddharthshailendra/Resume_Shortlister_using_llm_basic/sample_resume_2.txt\"\n",
    "    JOB_TITLE = \"machine learning engineer\"  \n",
    "    LOCATION = \"Mumbai\" \n",
    "    \n",
    "    print(\"üöÄ ENHANCED JOB SEARCH STARTING...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    ranked_jobs, resume_data = find_relevant_jobs(\n",
    "        YOUR_RESUME_PATH,\n",
    "        JOB_TITLE, \n",
    "        LOCATION\n",
    "    )\n",
    "    \n",
    "    if ranked_jobs:\n",
    "        print(\"‚úÖ Jobs analyzed successfully!\")\n",
    "        save_results_to_csv(ranked_jobs, \"my_job_recommendations.csv\")\n",
    "        \n",
    "        # Display results in clean format\n",
    "        display_csv_results_clean(\"my_job_recommendations.csv\")\n",
    "        \n",
    "        # Show insights\n",
    "        display_job_search_insights(\"my_job_recommendations.csv\")\n",
    "        \n",
    "        print(f\"\\nüéâ SEARCH COMPLETED! Found {len(ranked_jobs)} relevant jobs!\")\n",
    "        print(\"\\nüíº NEXT STEPS:\")\n",
    "        print(\"   1. Apply to top 3 jobs today\")\n",
    "        print(\"   2. Customize cover letters for each application\")\n",
    "        print(\"   3. Follow up in 5-7 days\")\n",
    "        print(\"   4. Track applications in a spreadsheet\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå No relevant jobs found.\")\n",
    "\n",
    "# Also add this function to view existing CSV files nicely\n",
    "def view_existing_jobs_clean(csv_file_path=\"my_job_recommendations.csv\"):\n",
    "    print(f\"üìÅ Viewing: {csv_file_path}\")\n",
    "    print(\"=\" * 70)\n",
    "    display_csv_results_clean(csv_file_path)\n",
    "    display_job_search_insights(csv_file_path)\n",
    "run_enhanced_job_search()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
