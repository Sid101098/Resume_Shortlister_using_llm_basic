{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccb6f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain-community chromadb sentence-transformers faiss-cpu requests pillow python-multipart opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8e70d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-docx in /opt/homebrew/lib/python3.11/site-packages (1.2.0)\n",
      "Requirement already satisfied: lxml>=3.1.0 in /opt/homebrew/lib/python3.11/site-packages (from python-docx) (6.0.2)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in /Users/siddharthshailendra/Library/Python/3.11/lib/python/site-packages (from python-docx) (4.15.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/opt/homebrew/opt/python@3.11/bin/python3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "import time\n",
    "import PyPDF2\n",
    "import docx\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98d02657",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightweightOllama:\n",
    "    def __init__(self, model_name=\"phi3:mini\"):\n",
    "        self.model_name = model_name\n",
    "        self.base_url = \"http://localhost:11434\"\n",
    "    \n",
    "    def invoke(self, prompt):\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                f\"{self.base_url}/api/generate\",\n",
    "                json={\n",
    "                    \"model\": self.model_name,\n",
    "                    \"prompt\": prompt,\n",
    "                    \"stream\": False,\n",
    "                    \"options\": {\n",
    "                        \"temperature\": 0.1,\n",
    "                        \"num_predict\": 200,\n",
    "                        \"top_k\": 20\n",
    "                    }\n",
    "                },\n",
    "                timeout=30\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                result = response.json().get(\"response\", \"\")\n",
    "                if \"@@@@\" in result or \"####\" in result:\n",
    "                    return \"Model response contained invalid characters. Please try a different model.\"\n",
    "                return result\n",
    "            else:\n",
    "                return f\"Error: {response.status_code}\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            return f\"Connection error: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5738a5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_lightweight_model():\n",
    "    try:\n",
    "        response = requests.get(\"http://localhost:11434/api/tags\", timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            models = response.json().get('models', [])\n",
    "            available_models = [model['name'] for model in models]\n",
    "            print(\"Available models:\", available_models)\n",
    "            preferred_models = ['phi3:mini', 'gemma2:2b', 'qwen2:0.5b', 'phi:latest', 'llama2:latest']\n",
    "            \n",
    "            for model in preferred_models:\n",
    "                if model in available_models:\n",
    "                    print(f\"âœ… Using {model} (lightweight)\")\n",
    "                    return model\n",
    "            \n",
    "            if available_models:\n",
    "                print(f\"âš ï¸ Using {available_models[0]} (fallback)\")\n",
    "                return available_models[0]\n",
    "                \n",
    "        return None\n",
    "    except:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d74205c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models: ['llava:latest', 'qwen2:0.5b', 'gemma2:2b', 'phi3:mini', 'phi:latest', 'llama2:latest']\n",
      "âœ… Using phi3:mini (lightweight)\n"
     ]
    }
   ],
   "source": [
    "model_name = get_best_lightweight_model()\n",
    "if model_name:\n",
    "    llm = LightweightOllama(model_name)\n",
    "else:\n",
    "    class SimpleLLM:\n",
    "        def invoke(self, prompt):\n",
    "            return \"Please install a lightweight model: ollama pull phi3:mini\"\n",
    "    llm = SimpleLLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0fcf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResumeParser:\n",
    "    def __init__(self):\n",
    "        self.skills_keywords = [\n",
    "            'python', 'java', 'javascript', 'sql', 'machine learning', 'deep learning',\n",
    "            'tensorflow', 'pytorch', 'react', 'node.js', 'aws', 'azure', 'docker',\n",
    "            'kubernetes', 'git', 'ci/cd', 'data analysis', 'tableau', 'power bi',\n",
    "            'mongodb', 'postgresql', 'mysql', 'html', 'css', 'rest api', 'graphql',\n",
    "            'agile', 'scrum', 'project management', 'leadership', 'communication',\n",
    "            'data science', 'artificial intelligence', 'ai', 'ml', 'nlp', 'computer vision',\n",
    "            'big data', 'spark', 'hadoop', 'kafka', 'redis', 'elasticsearch',\n",
    "            'jenkins', 'ansible', 'terraform', 'gcp', 'cloud', 'devops',\n",
    "            'frontend', 'backend', 'full stack', 'mobile', 'ios', 'android',\n",
    "            'flutter', 'react native', 'vue', 'angular', 'typescript'\n",
    "        ]\n",
    "    \n",
    "    def extract_text_from_pdf(self, pdf_path):\n",
    "        text = \"\"\n",
    "        try:\n",
    "            with open(pdf_path, 'rb') as file:\n",
    "                reader = PyPDF2.PdfReader(file)\n",
    "                for page in reader.pages:\n",
    "                    text += page.extract_text()\n",
    "            return text\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading PDF: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def extract_text_from_docx(self, docx_path):\n",
    "        try:\n",
    "            doc = docx.Document(docx_path)\n",
    "            text = \"\"\n",
    "            for paragraph in doc.paragraphs:\n",
    "                text += paragraph.text + \"\\n\"\n",
    "            return text\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading DOCX: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def parse_resume(self, resume_path):\n",
    "        print(f\"ğŸ“„ Parsing resume: {resume_path}\")\n",
    "        \n",
    "        if resume_path.endswith('.pdf'):\n",
    "            text = self.extract_text_from_pdf(resume_path)\n",
    "        elif resume_path.endswith('.docx'):\n",
    "            text = self.extract_text_from_docx(resume_path)\n",
    "        else:\n",
    "            try:\n",
    "                with open(resume_path, 'r', encoding='utf-8') as file:\n",
    "                    text = file.read()\n",
    "            except:\n",
    "                text = resume_path \n",
    "        \n",
    "        if not text.strip():\n",
    "            return {\"error\": \"Could not extract text from resume\"}\n",
    "       \n",
    "        prompt = f\"\"\"\n",
    "        Extract the following information from this resume text. Return ONLY a JSON object with these exact keys:\n",
    "        {{\n",
    "            \"name\": \"full name\",\n",
    "            \"email\": \"email address\", \n",
    "            \"phone\": \"phone number if available\",\n",
    "            \"summary\": \"brief professional summary\",\n",
    "            \"skills\": [\"list\", \"of\", \"technical\", \"skills\"],\n",
    "            \"experience\": \"years of experience\",\n",
    "            \"education\": \"highest education level\",\n",
    "            \"current_role\": \"current/most recent job title\",\n",
    "            \"industries\": [\"list\", \"of\", \"industries\", \"worked\", \"in\"]\n",
    "        }}\n",
    "        \n",
    "        RESUME TEXT:\n",
    "        {text[:3000]}\n",
    "        \n",
    "        Return ONLY the JSON object, nothing else.\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = llm.invoke(prompt)\n",
    "            response = response.strip()\n",
    "            if response.startswith('```json'):\n",
    "                response = response[7:]\n",
    "            if response.endswith('```'):\n",
    "                response = response[:-3]\n",
    "            \n",
    "            resume_data = json.loads(response)\n",
    "            resume_data['raw_text'] = text\n",
    "            return resume_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing with Ollama: {e}\")\n",
    "            return self.simple_parse(text)\n",
    "    \n",
    "    def simple_parse(self, text):\n",
    "        skills_found = []\n",
    "        for skill in self.skills_keywords:\n",
    "            if re.search(r'\\b' + re.escape(skill) + r'\\b', text.lower()):\n",
    "                skills_found.append(skill)\n",
    "  \n",
    "        email_match = re.search(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', text)\n",
    "        email = email_match.group() if email_match else \"Not found\"\n",
    "        \n",
    "        lines = text.split('\\n')\n",
    "        name = lines[0].strip() if lines else \"Not found\"\n",
    "        \n",
    "        return {\n",
    "            'name': name,\n",
    "            'email': email,\n",
    "            'skills': skills_found,\n",
    "            'summary': \"Extracted from resume\",\n",
    "            'raw_text': text\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad81e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JobScraper:\n",
    "    def __init__(self):\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "    \n",
    "    def scrape_indeed(self, job_title, location=\"\", limit=10):\n",
    "        jobs = []\n",
    "        try:\n",
    "            base_url = \"https://www.indeed.com/jobs\"\n",
    "            params = {\n",
    "                'q': job_title,\n",
    "                'l': location,\n",
    "                'limit': limit\n",
    "            }\n",
    "            \n",
    "            response = requests.get(base_url, params=params, headers=self.headers, timeout=10)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            job_cards = soup.find_all('div', class_='job_seen_beacon')\n",
    "            \n",
    "            for card in job_cards[:limit]:\n",
    "                try:\n",
    "                    title_elem = card.find('h2', class_='jobTitle')\n",
    "                    company_elem = card.find('span', class_='companyName')\n",
    "                    location_elem = card.find('div', class_='companyLocation')\n",
    "                    link_elem = card.find('a', class_='jcs-JobTitle')\n",
    "                    \n",
    "                    if title_elem and company_elem:\n",
    "                        job = {\n",
    "                            'title': title_elem.text.strip(),\n",
    "                            'company': company_elem.text.strip(),\n",
    "                            'location': location_elem.text.strip() if location_elem else \"Not specified\",\n",
    "                            'link': \"https://www.indeed.com\" + link_elem['href'] if link_elem else \"\",\n",
    "                            'source': 'Indeed',\n",
    "                            'description': self.get_job_description(\"https://www.indeed.com\" + link_elem['href']) if link_elem else \"\"\n",
    "                        }\n",
    "                        jobs.append(job)\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping Indeed: {e}\")\n",
    "            \n",
    "        return jobs\n",
    "    \n",
    "    def scrape_linkedin_simplified(self, job_title, location=\"\", limit=10):\n",
    "        jobs = []\n",
    "        try:\n",
    "            base_url = \"https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search\"\n",
    "            params = {\n",
    "                'keywords': job_title,\n",
    "                'location': location,\n",
    "                'start': 0\n",
    "            }\n",
    "            \n",
    "            response = requests.get(base_url, params=params, headers=self.headers, timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                \n",
    "                job_cards = soup.find_all('li')\n",
    "                \n",
    "                for card in job_cards[:limit]:\n",
    "                    try:\n",
    "                        title_elem = card.find('h3', class_='base-search-card__title')\n",
    "                        company_elem = card.find('h4', class_='base-search-card__subtitle')\n",
    "                        location_elem = card.find('span', class_='job-search-card__location')\n",
    "                        link_elem = card.find('a', class_='base-card__full-link')\n",
    "                        \n",
    "                        if title_elem and company_elem:\n",
    "                            job = {\n",
    "                                'title': title_elem.text.strip(),\n",
    "                                'company': company_elem.text.strip(),\n",
    "                                'location': location_elem.text.strip() if location_elem else \"Not specified\",\n",
    "                                'link': link_elem['href'] if link_elem else \"\",\n",
    "                                'source': 'LinkedIn',\n",
    "                                'description': \"\"\n",
    "                            }\n",
    "                            jobs.append(job)\n",
    "                    except Exception as e:\n",
    "                        continue\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping LinkedIn: {e}\")\n",
    "            \n",
    "        return jobs\n",
    "    \n",
    "    def get_job_description(self, job_url):\n",
    "        try:\n",
    "            response = requests.get(job_url, headers=self.headers, timeout=10)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            description_selectors = [\n",
    "                'div#jobDescriptionText',\n",
    "                'div.job-description',\n",
    "                'div.description',\n",
    "                'div.job-details'\n",
    "            ]\n",
    "            \n",
    "            for selector in description_selectors:\n",
    "                desc_elem = soup.select_one(selector)\n",
    "                if desc_elem:\n",
    "                    return desc_elem.get_text(strip=True)[:500]\n",
    "            \n",
    "            return \"Description not available\"\n",
    "        except:\n",
    "            return \"Description not available\"\n",
    "    \n",
    "    def search_multiple_sources(self, job_title, location=\"\", jobs_per_source=5):\n",
    "        all_jobs = []\n",
    "        \n",
    "        print(\"ğŸ” Searching Indeed...\")\n",
    "        indeed_jobs = self.scrape_indeed(job_title, location, jobs_per_source)\n",
    "        all_jobs.extend(indeed_jobs)\n",
    "        \n",
    "        print(\"ğŸ” Searching LinkedIn...\")\n",
    "        linkedin_jobs = self.scrape_linkedin_simplified(job_title, location, jobs_per_source)\n",
    "        all_jobs.extend(linkedin_jobs)\n",
    "        \n",
    "        return all_jobs[:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4689144",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JobMatcher:\n",
    "    def __init__(self):\n",
    "        self.llm = llm\n",
    "    \n",
    "    def calculate_similarity(self, resume_text, job_description):\n",
    "        try:\n",
    "            vectorizer = TfidfVectorizer()\n",
    "            tfidf_matrix = vectorizer.fit_transform([resume_text, job_description])\n",
    "            similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
    "            return similarity[0][0]\n",
    "        except:\n",
    "            return 0\n",
    "    \n",
    "    def analyze_job_fit(self, resume_data, job):\n",
    "        prompt = f\"\"\"\n",
    "        Analyze the match between a candidate and a job opportunity. Return ONLY a JSON response with this structure:\n",
    "        {{\n",
    "            \"relevance_score\": 0-100,\n",
    "            \"skills_match\": [\"list\", \"of\", \"matching\", \"skills\"],\n",
    "            \"missing_skills\": [\"list\", \"of\", \"missing\", \"important\", \"skills\"],\n",
    "            \"fit_analysis\": \"brief analysis of why this is a good fit\",\n",
    "            \"recommendation\": \"strong_recommend|recommend|neutral|not_recommend\"\n",
    "        }}\n",
    "        \n",
    "        CANDIDATE PROFILE:\n",
    "        Name: {resume_data.get('name', 'N/A')}\n",
    "        Skills: {', '.join(resume_data.get('skills', []))}\n",
    "        Experience: {resume_data.get('experience', 'N/A')}\n",
    "        Summary: {resume_data.get('summary', 'N/A')}\n",
    "        \n",
    "        JOB OPPORTUNITY:\n",
    "        Title: {job['title']}\n",
    "        Company: {job['company']}\n",
    "        Description: {job['description'][:1000]}\n",
    "        \n",
    "        Analyze the skills match, experience relevance, and overall fit.\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.llm.invoke(prompt)\n",
    "            response = response.strip()\n",
    "            if response.startswith('```json'):\n",
    "                response = response[7:]\n",
    "            if response.endswith('```'):\n",
    "                response = response[:-3]\n",
    "            \n",
    "            analysis = json.loads(response)\n",
    "            \n",
    "            analysis['text_similarity'] = self.calculate_similarity(\n",
    "                resume_data.get('raw_text', ''),\n",
    "                job['description']\n",
    "            )\n",
    "            \n",
    "            return analysis\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in job fit analysis: {e}\")\n",
    "            return {\n",
    "                \"relevance_score\": 50,\n",
    "                \"skills_match\": [],\n",
    "                \"missing_skills\": [],\n",
    "                \"fit_analysis\": \"Analysis unavailable\",\n",
    "                \"recommendation\": \"neutral\",\n",
    "                \"text_similarity\": 0\n",
    "            }\n",
    "    \n",
    "    def rank_jobs(self, resume_data, jobs):\n",
    "        ranked_jobs = []\n",
    "        \n",
    "        print(\"ğŸ¯ Analyzing job matches...\")\n",
    "        for i, job in enumerate(jobs):\n",
    "            print(f\"  Analyzing job {i+1}/{len(jobs)}...\")\n",
    "            \n",
    "            analysis = self.analyze_job_fit(resume_data, job)\n",
    "            \n",
    "            overall_score = (analysis['relevance_score'] * 0.7 + \n",
    "                           analysis['text_similarity'] * 100 * 0.3)\n",
    "            \n",
    "            ranked_jobs.append({\n",
    "                **job,\n",
    "                'analysis': analysis,\n",
    "                'overall_score': overall_score\n",
    "            })\n",
    "            \n",
    "          \n",
    "            time.sleep(1)\n",
    "        \n",
    "      \n",
    "        ranked_jobs.sort(key=lambda x: x['overall_score'], reverse=True)\n",
    "        return ranked_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3d36fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_relevant_jobs(resume_path, job_title, location=\"\", num_jobs=10):\n",
    "    \"\"\"\n",
    "    Main function to find relevant jobs based on resume\n",
    "    \"\"\"\n",
    "    print(\"ğŸš€ Starting Job Search Agent...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "\n",
    "    parser = ResumeParser()\n",
    "    resume_data = parser.parse_resume(resume_path)\n",
    "    \n",
    "    if 'error' in resume_data:\n",
    "        print(f\"âŒ Error: {resume_data['error']}\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"âœ… Resume parsed for: {resume_data.get('name', 'Unknown')}\")\n",
    "    print(f\"ğŸ“§ Email: {resume_data.get('email', 'Not found')}\")\n",
    "    print(f\"ğŸ› ï¸ Skills: {', '.join(resume_data.get('skills', []))}\")\n",
    "    print()\n",
    "   \n",
    "    scraper = JobScraper()\n",
    "    jobs = scraper.search_multiple_sources(job_title, location, num_jobs//2)\n",
    "    \n",
    "    if not jobs:\n",
    "        print(\"âŒ No jobs found. Try different search terms.\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"âœ… Found {len(jobs)} jobs to analyze\")\n",
    "    print()\n",
    "    matcher = JobMatcher()\n",
    "    ranked_jobs = matcher.rank_jobs(resume_data, jobs)\n",
    "    \n",
    "    return ranked_jobs, resume_data\n",
    "\n",
    "def display_results(ranked_jobs, resume_data, top_k=10):\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ğŸ¯ TOP JOB RECOMMENDATIONS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for i, job in enumerate(ranked_jobs[:top_k]):\n",
    "        print(f\"\\nğŸ† #{i+1} | Score: {job['overall_score']:.1f}/100\")\n",
    "        print(f\"ğŸ“Œ Title: {job['title']}\")\n",
    "        print(f\"ğŸ¢ Company: {job['company']}\")\n",
    "        print(f\"ğŸ“ Location: {job['location']}\")\n",
    "        print(f\"ğŸ”— Source: {job['source']}\")\n",
    "        print(f\"ğŸ”— Link: {job['link']}\")\n",
    "        \n",
    "        analysis = job['analysis']\n",
    "        print(f\"âœ… Matching Skills: {', '.join(analysis['skills_match'][:5])}\")\n",
    "        if analysis['missing_skills']:\n",
    "            print(f\"âš ï¸ Missing: {', '.join(analysis['missing_skills'][:3])}\")\n",
    "        print(f\"ğŸ“Š Analysis: {analysis['fit_analysis']}\")\n",
    "        print(f\"ğŸ’¡ Recommendation: {analysis['recommendation'].replace('_', ' ').title()}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "def save_results_to_csv(ranked_jobs, filename=\"job_recommendations.csv\"):\n",
    "    data = []\n",
    "    for job in ranked_jobs:\n",
    "        data.append({\n",
    "            'Title': job['title'],\n",
    "            'Company': job['company'],\n",
    "            'Location': job['location'],\n",
    "            'Score': f\"{job['overall_score']:.1f}\",\n",
    "            'Link': job['link'],\n",
    "            'Source': job['source'],\n",
    "            'Matching Skills': ', '.join(job['analysis']['skills_match'][:5]),\n",
    "            'Missing Skills': ', '.join(job['analysis']['missing_skills'][:3]),\n",
    "            'Analysis': job['analysis']['fit_analysis']\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"ğŸ’¾ Results saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49610162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª Testing Job Search System...\n",
      "ğŸš€ Starting Job Search Agent...\n",
      "==================================================\n",
      "ğŸ“„ Parsing resume: sample_resume.txt\n",
      "Error parsing with Ollama: Unterminated string starting at: line 43 column 18 (char 519)\n",
      "âœ… Resume parsed for: \n",
      "ğŸ“§ Email: rhea.1@email.com\n",
      "ğŸ› ï¸ Skills: python, sql, machine learning, deep learning, tensorflow, pytorch, aws, docker, kubernetes, git, data analysis, ml, nlp, cloud\n",
      "\n",
      "ğŸ” Searching Indeed...\n",
      "ğŸ” Searching LinkedIn...\n",
      "âœ… Found 5 jobs to analyze\n",
      "\n",
      "ğŸ¯ Analyzing job matches...\n",
      "  Analyzing job 1/5...\n",
      "  Analyzing job 2/5...\n",
      "  Analyzing job 3/5...\n",
      "  Analyzing job 4/5...\n",
      "  Analyzing job 5/5...\n",
      "\n",
      "================================================================================\n",
      "ğŸ¯ TOP JOB RECOMMENDATIONS\n",
      "================================================================================\n",
      "\n",
      "ğŸ† #1 | Score: 59.5/100\n",
      "ğŸ“Œ Title: Data Scientist- Across PAN India\n",
      "ğŸ¢ Company: Capgemini Engineering\n",
      "ğŸ“ Location: Bengaluru, Karnataka, India\n",
      "ğŸ”— Source: LinkedIn\n",
      "ğŸ”— Link: https://in.linkedin.com/jobs/view/data-scientist-across-pan-india-at-capgemini-engineering-4296207682?position=1&pageNum=0&refId=C82IrQTox%2BW7TiOJ8%2BITIQ%3D%3D&trackingId=vhu8TAxDAPxrFUxA7%2BlWUA%3D%3D\n",
      "âœ… Matching Skills: python, sql, machine learning, deep learning, tensorflow\n",
      "âš ï¸ Missing: experience in data science field, proven track record with large datasets, knowledge of specific industry applications relevant to Capgemini Engineering's projects\n",
      "ğŸ“Š Analysis: The candidate has a strong technical background and skills set that align well with the job description. However, lacking direct experience may require additional training or mentorship.\n",
      "ğŸ’¡ Recommendation: Strong Recommend\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ğŸ† #2 | Score: 52.5/100\n",
      "ğŸ“Œ Title: AI / ML Engineer\n",
      "ğŸ¢ Company: HDFC Bank\n",
      "ğŸ“ Location: Bengaluru, Karnataka, India\n",
      "ğŸ”— Source: LinkedIn\n",
      "ğŸ”— Link: https://in.linkedin.com/jobs/view/ai-ml-engineer-at-hdfc-bank-4302326597?position=2&pageNum=0&refId=C82IrQTox%2BW7TiOJ8%2BITIQ%3D%3D&trackingId=ZYXMki9Opjv2Y%2BhyaaK3%2Fw%3D%3D\n",
      "âœ… Matching Skills: python, sql, machine learning, deep learning, tensorflow\n",
      "âš ï¸ Missing: experience in finance or banking industry, knowledge of HDFC Bank's specific systems and processes\n",
      "ğŸ“Š Analysis: The candidate has a strong technical background with skills relevant to AI/ML engineering. However, the lack of experience within the financial sector may be a concern for this role at HDFC Bank.\n",
      "ğŸ’¡ Recommendation: Neutral\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ğŸ† #3 | Score: 52.5/100\n",
      "ğŸ“Œ Title: Staff AI Scientist\n",
      "ğŸ¢ Company: Intuit\n",
      "ğŸ“ Location: Bengaluru, Karnataka, India\n",
      "ğŸ”— Source: LinkedIn\n",
      "ğŸ”— Link: https://in.linkedin.com/jobs/view/staff-ai-scientist-at-intuit-4303899259?position=3&pageNum=0&refId=C82IrQTox%2BW7TiOJ8%2BITIQ%3D%3D&trackingId=c411lwW%2BIdotC%2Bi3dzD3sQ%3D%3D\n",
      "âœ… Matching Skills: python, machine learning, deep learning, aws, git\n",
      "âš ï¸ Missing: experience in AI field, specific knowledge of Intuit's products and services\n",
      "ğŸ“Š Analysis: The candidate has a strong foundation in technical skills relevant to the job, including Python programming languages used for machine learning tasks. Their experience with cloud platforms like AWS aligns well with industry standards.\n",
      "ğŸ’¡ Recommendation: Strong Recommend\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ğŸ† #4 | Score: 52.5/100\n",
      "ğŸ“Œ Title: Data Scientist\n",
      "ğŸ¢ Company: Microsoft\n",
      "ğŸ“ Location: Bengaluru, Karnataka, India\n",
      "ğŸ”— Source: LinkedIn\n",
      "ğŸ”— Link: https://in.linkedin.com/jobs/view/data-scientist-at-microsoft-4311367673?position=4&pageNum=0&refId=C82IrQTox%2BW7TiOJ8%2BITIQ%3D%3D&trackingId=aWISfSKXGhCVVSKoomYTiQ%3D%3D\n",
      "âœ… Matching Skills: python, sql, machine learning, deep learning, aws\n",
      "âš ï¸ Missing: experience in data science field, proven track record of successful projects or research work related to the job description\n",
      "ğŸ“Š Analysis: The candidate has a strong set of technical skills relevant for a Data Scientist role, including machine learning and cloud services. However, without experience specifically mentioned as 'data scientist' in their profile.\n",
      "ğŸ’¡ Recommendation: Strong Recommend\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ğŸ† #5 | Score: 52.5/100\n",
      "ğŸ“Œ Title: Data Scientist\n",
      "ğŸ¢ Company: AB InBev GCC India\n",
      "ğŸ“ Location: Bengaluru, Karnataka, India\n",
      "ğŸ”— Source: LinkedIn\n",
      "ğŸ”— Link: https://in.linkedin.com/jobs/view/data-scientist-at-ab-inbev-gcc-india-4304090614?position=5&pageNum=0&refId=C82IrQTox%2BW7TiOJ8%2BITIQ%3D%3D&trackingId=0qY7tZupE28sfD0ZBhF8Cw%3D%3D\n",
      "âœ… Matching Skills: python, sql, machine learning, deep learning, tensorflow\n",
      "âš ï¸ Missing: experience in the beverage industry, knowledge of AB InBev's specific data science needs and practices\n",
      "ğŸ“Š Analysis: The candidate has a strong technical background with skills relevant to machine learning which is often required for Data Scientist roles. However, lacking direct experience within the target company or its sector may affect their fit.\n",
      "ğŸ’¡ Recommendation: Neutral\n",
      "--------------------------------------------------------------------------------\n",
      "ğŸ’¾ Results saved to job_recommendations.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def test_system():\n",
    "\n",
    "    sample_resume_text = \"\"\"\n",
    "    Rhea\n",
    "    Senior Data Scientist\n",
    "    rhea.1@email.com | 1122334455 | linkedin.com/in/rhea\n",
    "    \n",
    "    SUMMARY:\n",
    "    Experienced Data Scientist with 5+ years in machine learning, NLP, and cloud technologies. \n",
    "    Strong background in Python, TensorFlow, and AWS.\n",
    "    \n",
    "    EXPERIENCE:\n",
    "    Senior Data Scientist - TechCorp (2020-Present)\n",
    "    - Led ML projects improving customer recommendations by 30%\n",
    "    - Developed NLP models for sentiment analysis\n",
    "    - Managed AWS infrastructure for ML pipelines\n",
    "    \n",
    "    Data Scientist - DataWorks (2018-2020)\n",
    "    - Built predictive models using Python and Scikit-learn\n",
    "    - Implemented deep learning solutions with TensorFlow\n",
    "    \n",
    "    SKILLS:\n",
    "    Python, Machine Learning, Deep Learning, TensorFlow, PyTorch, NLP, \n",
    "    AWS, SQL, Docker, Kubernetes, Git, Data Analysis\n",
    "    \n",
    "    EDUCATION:\n",
    "    MS Computer Science - IIT\n",
    "    BS Mathematics - VIT\n",
    "    \"\"\"\n",
    "\n",
    "    with open(\"sample_resume.txt\", \"w\") as f:\n",
    "        f.write(sample_resume_text)\n",
    "  \n",
    "    job_title = \"data scientist\"\n",
    "    location = \"Bangalore\"\n",
    "    \n",
    "    ranked_jobs, resume_data = find_relevant_jobs(\n",
    "        \"sample_resume.txt\", \n",
    "        job_title, \n",
    "        location\n",
    "    )\n",
    "    \n",
    "    if ranked_jobs:\n",
    "        display_results(ranked_jobs, resume_data)\n",
    "        save_results_to_csv(ranked_jobs)\n",
    "    else:\n",
    "        print(\"âŒ No relevant jobs found.\")\n",
    "print(\"ğŸ§ª Testing Job Search System...\")\n",
    "test_system()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39ab7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Starting job search with your resume...\n",
      "ğŸš€ Starting Job Search Agent...\n",
      "==================================================\n",
      "ğŸ“„ Parsing resume: /Users/siddharthshailendra/Resume_Shortlister_using_llm_basic/sample_resume_2.txt\n",
      "Error parsing with Ollama: Expecting value: line 41 column 17 (char 520)\n",
      "âœ… Resume parsed for: Arjun Sharma\n",
      "ğŸ“§ Email: arjun.sharma@email.com\n",
      "ğŸ› ï¸ Skills: python, sql, machine learning, deep learning, tensorflow, pytorch, aws, docker, kubernetes, git, ci/cd, artificial intelligence, ai, ml, nlp, computer vision, spark, kafka, gcp, cloud, mobile\n",
      "\n",
      "ğŸ” Searching Indeed...\n",
      "ğŸ” Searching LinkedIn...\n",
      "âœ… Found 5 jobs to analyze\n",
      "\n",
      "ğŸ¯ Analyzing job matches...\n",
      "  Analyzing job 1/5...\n",
      "Error in job fit analysis: Unterminated string starting at: line 13 column 5 (char 717)\n",
      "  Analyzing job 2/5...\n",
      "  Analyzing job 3/5...\n",
      "  Analyzing job 4/5...\n",
      "  Analyzing job 5/5...\n",
      "\n",
      "================================================================================\n",
      "ğŸ¯ TOP JOB RECOMMENDATIONS\n",
      "================================================================================\n",
      "\n",
      "ğŸ† #1 | Score: 59.5/100\n",
      "ğŸ“Œ Title: Data scientist- Python- AI/ML GEN AI- Across india\n",
      "ğŸ¢ Company: Capgemini Engineering\n",
      "ğŸ“ Location: Mumbai, Maharashtra, India\n",
      "ğŸ”— Source: LinkedIn\n",
      "ğŸ”— Link: https://in.linkedin.com/jobs/view/data-scientist-python-ai-ml-gen-ai-across-india-at-capgemini-engineering-4295891624?position=3&pageNum=0&refId=9ogJbiF7OtKPPrXN6mMidw%3D%3D&trackingId=iPqgf0aR7Nn2WQM%2Ba6%2FKlA%3D%3D\n",
      "âœ… Matching Skills: python, machine learning, deep learning, tensorflow, pytorch\n",
      "âš ï¸ Missing: experience in data science field, knowledge of spark and kafka for big data processing, understanding of gcp services beyond basic knowledge\n",
      "ğŸ“Š Analysis: Arjun Sharma has a strong technical background with skills relevant to the job description. His expertise aligns well with Capgemini Engineering's focus on Python, AI/ML and cloud technologies.\n",
      "ğŸ’¡ Recommendation: Strong Recommend\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ğŸ† #2 | Score: 52.5/100\n",
      "ğŸ“Œ Title: Data Scientist\n",
      "ğŸ¢ Company: Deloitte\n",
      "ğŸ“ Location: Mumbai, Maharashtra, India\n",
      "ğŸ”— Source: LinkedIn\n",
      "ğŸ”— Link: https://in.linkedin.com/jobs/view/data-scientist-at-deloitte-4307550628?position=2&pageNum=0&refId=9ogJbiF7OtKPPrXN6mMidw%3D%3D&trackingId=bgZBL%2Blt8Ddiiq%2Bi7TJizA%3D%3D\n",
      "âœ… Matching Skills: python, sql, machine learning, deep learning, tensorflow\n",
      "âš ï¸ Missing: experience in data science field, proven track record with big datasets, knowledge of statistical analysis methods\n",
      "ğŸ“Š Analysis: Arjun Sharma has a strong technical skill set that aligns well with the job requirements for Deloitte's Data Scientist position. His expertise in machine learning, deep learning frameworks like TensorFlow and PyTorch, as well as cloud services such as AWS are highly relevant to data science roles.\n",
      "ğŸ’¡ Recommendation: Strong Recommend\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ğŸ† #3 | Score: 52.5/100\n",
      "ğŸ“Œ Title: Data Scientist\n",
      "ğŸ¢ Company: Procter & Gamble\n",
      "ğŸ“ Location: Mumbai, Maharashtra, India\n",
      "ğŸ”— Source: LinkedIn\n",
      "ğŸ”— Link: https://in.linkedin.com/jobs/view/data-scientist-at-procter-gamble-4303485542?position=4&pageNum=0&refId=9ogJbiF7OtKPPrXN6mMidw%3D%3D&trackingId=DIpGi9X8f4t6nPnOKBdpSQ%3D%3D\n",
      "âœ… Matching Skills: python, sql, machine learning, deep learning, tensorflow\n",
      "âš ï¸ Missing: experience in data science field, proven track record with large datasets and complex analytics projects, familiarity with specific industry applications relevant to Procter & Gamble's products or services\n",
      "ğŸ“Š Analysis: Arjun Sharma has a strong technical background that aligns well with the data science role at P&G. His skills in machine learning, deep learning, and AI are particularly valuable for innovation within this industry.\n",
      "ğŸ’¡ Recommendation: Strong Recommend\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ğŸ† #4 | Score: 52.5/100\n",
      "ğŸ“Œ Title: AI/MLâ€“ Applied Artificial Intelligence & Machine Learning Internship in Mumbai (Hybrid)\n",
      "ğŸ¢ Company: Reflex Realty LLP\n",
      "ğŸ“ Location: Mumbai Metropolitan Region\n",
      "ğŸ”— Source: LinkedIn\n",
      "ğŸ”— Link: https://in.linkedin.com/jobs/view/ai-ml%E2%80%93-applied-artificial-intelligence-machine-learning-internship-in-mumbai-hybrid-at-reflex-realty-llp-4295853106?position=5&pageNum=0&refId=9ogJbiF7OtKPPrXN6mMidw%3D%3D&trackingId=v8jZfIqNtY%2FXOC%2F%2FyuaqnQ%3D%3D\n",
      "âœ… Matching Skills: python, machine learning, artificial intelligence\n",
      "âš ï¸ Missing: experience in AI/ML field, specific knowledge of cloud services like aws and gcp beyond general awareness\n",
      "ğŸ“Š Analysis: Arjun Sharma has a strong foundation in programming languages, machine learning frameworks (tensorflow, pytorch), as well as some experience with containerization technologies (docker) which are relevant to the hybrid work environment of Reflex Realty LLP. However, his lack of direct AI/ML industry experience might be a concern for this internship role.\n",
      "ğŸ’¡ Recommendation: Neutral\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ğŸ† #5 | Score: 35.0/100\n",
      "ğŸ“Œ Title: Data Scientist\n",
      "ğŸ¢ Company: Deloitte\n",
      "ğŸ“ Location: Mumbai, Maharashtra, India\n",
      "ğŸ”— Source: LinkedIn\n",
      "ğŸ”— Link: https://in.linkedin.com/jobs/view/data-scientist-at-deloitte-4293182219?position=1&pageNum=0&refId=9ogJbiF7OtKPPrXN6mMidw%3D%3D&trackingId=k6TUpQEPKxW2g7%2Fq5f2C9w%3D%3D\n",
      "âœ… Matching Skills: \n",
      "ğŸ“Š Analysis: Analysis unavailable\n",
      "ğŸ’¡ Recommendation: Neutral\n",
      "--------------------------------------------------------------------------------\n",
      "ğŸ’¾ Results saved to my_job_recommendations.csv\n",
      "\n",
      "ğŸ‰ Found 5 relevant jobs!\n",
      "ğŸ’¡ Tips:\n",
      "  - Apply to top 3-5 jobs first\n",
      "  - Customize your resume for each application\n",
      "  - Use the analysis to improve your resume\n"
     ]
    }
   ],
   "source": [
    "def search_jobs_with_your_resume():\n",
    "    YOUR_RESUME_PATH = \"/Users/siddharthshailendra/Resume_Shortlister_using_llm_basic/sample_resume_2.txt\"  # Remove extra slash\n",
    "    JOB_TITLE = \"machine learning engineer\"  \n",
    "    LOCATION = \"Mumbai\" \n",
    "    \n",
    "    print(\"ğŸ” Starting job search with your resume...\")\n",
    "    ranked_jobs, resume_data = find_relevant_jobs(\n",
    "        YOUR_RESUME_PATH,\n",
    "        JOB_TITLE, \n",
    "        LOCATION\n",
    "    )\n",
    "    \n",
    "    if ranked_jobs:\n",
    "        display_results(ranked_jobs, resume_data)\n",
    "        save_results_to_csv(ranked_jobs, \"my_job_recommendations.csv\")\n",
    "        \n",
    "        print(f\"\\nğŸ‰ Found {len(ranked_jobs)} relevant jobs!\")\n",
    "        print(\"ğŸ’¡ Tips:\")\n",
    "        print(\"  - Apply to top 3-5 jobs first\")\n",
    "        print(\"  - Customize your resume for each application\")\n",
    "        print(\"  - Use the analysis to improve your resume\")\n",
    "    else:\n",
    "        print(\"âŒ No relevant jobs found. Try:\")\n",
    "        print(\"  - Different job titles\")\n",
    "        print(\"  - Broader location search\")\n",
    "        print(\"  - Check your resume format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43f2cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ ENHANCED JOB SEARCH STARTING...\n",
      "============================================================\n",
      "ğŸš€ Starting Job Search Agent...\n",
      "==================================================\n",
      "ğŸ“„ Parsing resume: /Users/siddharthshailendra/Resume_Shortlister_using_llm_basic/sample_resume_2.txt\n",
      "Error parsing with Ollama: Expecting value: line 41 column 17 (char 520)\n",
      "âœ… Resume parsed for: Arjun Sharma\n",
      "ğŸ“§ Email: arjun.sharma@email.com\n",
      "ğŸ› ï¸ Skills: python, sql, machine learning, deep learning, tensorflow, pytorch, aws, docker, kubernetes, git, ci/cd, artificial intelligence, ai, ml, nlp, computer vision, spark, kafka, gcp, cloud, mobile\n",
      "\n",
      "ğŸ” Searching Indeed...\n",
      "ğŸ” Searching LinkedIn...\n",
      "âœ… Found 5 jobs to analyze\n",
      "\n",
      "ğŸ¯ Analyzing job matches...\n",
      "  Analyzing job 1/5...\n",
      "  Analyzing job 2/5...\n",
      "  Analyzing job 3/5...\n",
      "  Analyzing job 4/5...\n",
      "  Analyzing job 5/5...\n",
      "âœ… Jobs analyzed successfully!\n",
      "ğŸ’¾ Results saved to my_job_recommendations.csv\n",
      "ğŸ¯ JOB SEARCH RESULTS SUMMARY\n",
      "======================================================================\n",
      "ğŸ“Š Total Jobs Found: 5\n",
      "ğŸ† Average Score: 53.9/100\n",
      "ğŸ’¼ Top Companies: Capgemini Engineering, Deloitte, Deloitte\n",
      "\n",
      "ğŸ… RANK #1 | SCORE: 59.5/100\n",
      "ğŸ“Œ Data scientist- Python- AI/ML GEN AI- Across india\n",
      "ğŸ¢ Capgemini Engineering | ğŸ“ Mumbai, Maharashtra, India\n",
      "ğŸ”— https://in.linkedin.com/jobs/view/data-scientist-python-ai-ml-gen-ai-across-india-at-capgemini-engineering-4295891624?position=3&pageNum=0&refId=swnV5erkbv6Hq8y5pQ%2BnJw%3D%3D&trackingId=ALJXuG2zruaBKVvy24tYpA%3D%3D\n",
      "ğŸ“± Source: LinkedIn\n",
      "âœ… Matching Skills: python, machine learning, deep learning, tensorflow, pytorch\n",
      "âš ï¸  Missing Skills: experience in data science field, knowledge of spark and kafka, understanding of\n",
      "gcp services\n",
      "ğŸ“‹ Analysis: Arjun Sharma has a strong foundation in Python, machine learning, deep learning, AI/ML tools like\n",
      "TensorFlow and PyTorch, as well as cloud technologies such as AWS. His skills align closely with the\n",
      "job requirements for an entry-level Data Scientist role at Capgemini Engineering.\n",
      "ğŸ’¡ Recommendation: Strong Recommend\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ… RANK #2 | SCORE: 52.5/100\n",
      "ğŸ“Œ Data Scientist\n",
      "ğŸ¢ Deloitte | ğŸ“ Mumbai, Maharashtra, India\n",
      "ğŸ”— https://in.linkedin.com/jobs/view/data-scientist-at-deloitte-4293182219?position=1&pageNum=0&refId=swnV5erkbv6Hq8y5pQ%2BnJw%3D%3D&trackingId=%2FsVMwlY74d7Ibe0P470NhA%3D%3D\n",
      "ğŸ“± Source: LinkedIn\n",
      "âœ… Matching Skills: python, sql, machine learning, deep learning, tensorflow\n",
      "âš ï¸  Missing Skills: experience in data science field, proven track record with big datasets,\n",
      "knowledge of statistical analysis methods\n",
      "ğŸ“‹ Analysis: Arjun Sharma has a strong technical background and is proficient in several skills relevant to the\n",
      "Data Scientist role at Deloitte. However, his lack of experience directly related to data science\n",
      "may be seen as a gap.\n",
      "ğŸ’¡ Recommendation: Neutral\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ… RANK #3 | SCORE: 52.5/100\n",
      "ğŸ“Œ Data Scientist\n",
      "ğŸ¢ Deloitte | ğŸ“ Mumbai, Maharashtra, India\n",
      "ğŸ”— https://in.linkedin.com/jobs/view/data-scientist-at-deloitte-4307550628?position=2&pageNum=0&refId=swnV5erkbv6Hq8y5pQ%2BnJw%3D%3D&trackingId=RijiaT08vFBb4x239hfS2g%3D%3D\n",
      "ğŸ“± Source: LinkedIn\n",
      "âœ… Matching Skills: python, sql, machine learning, deep learning, tensorflow\n",
      "âš ï¸  Missing Skills: experience in data science field, proven track record with big datasets,\n",
      "knowledge of statistical analysis methods\n",
      "ğŸ“‹ Analysis: Arjun Sharma has a strong technical skill set that aligns well with the job description for Data\n",
      "Scientist at Deloitte. His expertise in machine learning, deep learning frameworks like TensorFlow\n",
      "and PyTorch, as well as cloud services such as AWS are highly relevant to data science roles.\n",
      "ğŸ’¡ Recommendation: Strong Recommend\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ… RANK #4 | SCORE: 52.5/100\n",
      "ğŸ“Œ Data Scientist\n",
      "ğŸ¢ Procter & Gamble | ğŸ“ Mumbai, Maharashtra, India\n",
      "ğŸ”— https://in.linkedin.com/jobs/view/data-scientist-at-procter-gamble-4303485542?position=4&pageNum=0&refId=swnV5erkbv6Hq8y5pQ%2BnJw%3D%3D&trackingId=hfqfFetBZaj4ksJhWQqnIg%3D%3D\n",
      "ğŸ“± Source: LinkedIn\n",
      "âœ… Matching Skills: python, sql, machine learning, deep learning, tensorflow\n",
      "âš ï¸  Missing Skills: experience in data science field, procter & gamble specific industry knowledge\n",
      "ğŸ“‹ Analysis: Arjun Sharma has a strong technical background with skills relevant to the Data Scientist role at\n",
      "Procter & Gamble. However, his lack of direct experience within the company or its sector may\n",
      "require additional on-the-job learning.\n",
      "ğŸ’¡ Recommendation: Neutral\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ… RANK #5 | SCORE: 52.5/100\n",
      "ğŸ“Œ AI/MLâ€“ Applied Artificial Intelligence & Machine Learning Internship in Mumbai (Hybrid)\n",
      "ğŸ¢ Reflex Realty LLP | ğŸ“ Mumbai Metropolitan Region\n",
      "ğŸ”— https://in.linkedin.com/jobs/view/ai-ml%E2%80%93-applied-artificial-intelligence-machine-learning-internship-in-mumbai-hybrid-at-reflex-realty-llp-4295853106?position=5&pageNum=0&refId=swnV5erkbv6Hq8y5pQ%2BnJw%3D%3D&trackingId=ZBFI%2BevGf9SthslHabUTSg%3D%3D\n",
      "ğŸ“± Source: LinkedIn\n",
      "âœ… Matching Skills: python, machine learning, artificial intelligence\n",
      "âš ï¸  Missing Skills: experience in AI/ML field, specific knowledge of cloud services used by Reflex\n",
      "Realty LLP (if different from aws), understanding of the local market and\n",
      "industry specifics for Mumbai region\n",
      "ğŸ“‹ Analysis: Arjun Sharma has a strong foundation in machine learning, artificial intelligence, and related\n",
      "programming languages which align with the job's technical requirements. However, his lack of\n",
      "experience is noticeable.\n",
      "ğŸ’¡ Recommendation: Neutral\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ“ˆ JOB SEARCH INSIGHTS\n",
      "============================================================\n",
      "ğŸ¯ Average Match Score: 53.9/100\n",
      "ğŸ† Highest Score: 59.5/100\n",
      "ğŸ“Š Score Range: 52.5 - 59.5\n",
      "\n",
      "ğŸ“ˆ SCORE DISTRIBUTION:\n",
      "   ğŸ¯ Excellent (80-100): 0 jobs\n",
      "   âœ… Good (60-79): 0 jobs\n",
      "   âš ï¸  Average (40-59): 5 jobs\n",
      "   ğŸ”´ Low (0-39): 0 jobs\n",
      "\n",
      "ğŸ¢ TOP COMPANIES:\n",
      "   â€¢ Deloitte: 2 jobs (avg: 52.5/100)\n",
      "   â€¢ Capgemini Engineering: 1 jobs (avg: 59.5/100)\n",
      "   â€¢ Procter & Gamble: 1 jobs (avg: 52.5/100)\n",
      "\n",
      "ğŸ› ï¸  TOP MATCHING SKILLS:\n",
      "   â€¢ python: 5 jobs\n",
      "   â€¢ machine learning: 5 jobs\n",
      "   â€¢ deep learning: 4 jobs\n",
      "   â€¢ tensorflow: 4 jobs\n",
      "   â€¢ sql: 3 jobs\n",
      "\n",
      "ğŸ“š SKILLS TO IMPROVE:\n",
      "   â€¢ experience in data science field: missing in 4 jobs\n",
      "   â€¢ proven track record with big datasets: missing in 2 jobs\n",
      "   â€¢ knowledge of statistical analysis methods: missing in 2 jobs\n",
      "\n",
      "ğŸ’¡ ACTION PLAN:\n",
      "   ğŸ¯ Apply to top 3 jobs based on score\n",
      "   ğŸ“š Focus on learning: experience in data science field\n",
      "   â° Apply within 48 hours for best results\n",
      "\n",
      "ğŸ‰ SEARCH COMPLETED! Found 5 relevant jobs!\n",
      "\n",
      "ğŸ’¼ NEXT STEPS:\n",
      "   1. Apply to top 3 jobs today\n",
      "   2. Customize cover letters for each application\n",
      "   3. Follow up in 5-7 days\n",
      "   4. Track applications in a spreadsheet\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, Markdown\n",
    "import textwrap\n",
    "\n",
    "def display_csv_results_clean(csv_file_path=\"my_job_recommendations.csv\"):\n",
    "    \"\"\"\n",
    "    Clean display of CSV results without truncation\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "        \n",
    "        print(\"ğŸ¯ JOB SEARCH RESULTS SUMMARY\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"ğŸ“Š Total Jobs Found: {len(df)}\")\n",
    "        print(f\"ğŸ† Average Score: {df['Score'].mean():.1f}/100\")\n",
    "        print(f\"ğŸ’¼ Top Companies: {', '.join(df['Company'].head(3).tolist())}\")\n",
    "        print()\n",
    "    \n",
    "        for idx, row in df.iterrows():\n",
    "            print(f\"ğŸ… RANK #{idx+1} | SCORE: {row['Score']}/100\")\n",
    "            print(f\"ğŸ“Œ {row['Title']}\")\n",
    "            print(f\"ğŸ¢ {row['Company']} | ğŸ“ {row['Location']}\")\n",
    "            print(f\"ğŸ”— {row['Link']}\")\n",
    "            print(f\"ğŸ“± Source: {row['Source']}\")\n",
    "            \n",
    "            matching_skills = str(row['Matching Skills'])\n",
    "            if len(matching_skills) > 80:\n",
    "                matching_skills = textwrap.fill(matching_skills, width=80)\n",
    "            \n",
    "            print(f\"âœ… Matching Skills: {matching_skills}\")\n",
    "            \n",
    "            missing_skills = str(row['Missing Skills'])\n",
    "            if missing_skills != 'nan' and missing_skills.strip():\n",
    "                if len(missing_skills) > 80:\n",
    "                    missing_skills = textwrap.fill(missing_skills, width=80)\n",
    "                print(f\"âš ï¸  Missing Skills: {missing_skills}\")\n",
    "        \n",
    "            analysis = str(row['Analysis'])\n",
    "            if len(analysis) > 100:\n",
    "                analysis = textwrap.fill(analysis, width=100)\n",
    "            print(f\"ğŸ“‹ Analysis: {analysis}\")\n",
    "            \n",
    "            print(f\"ğŸ’¡ Recommendation: {row['Recommendation']}\")\n",
    "            print(\"â”€\" * 70)\n",
    "            print()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error displaying results: {e}\")\n",
    "\n",
    "def display_job_search_insights(csv_file_path=\"my_job_recommendations.csv\"):\n",
    "    \"\"\"\n",
    "    Show insights and analytics from the job search\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "        \n",
    "        print(\"ğŸ“ˆ JOB SEARCH INSIGHTS\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        df['Score_Numeric'] = pd.to_numeric(df['Score'], errors='coerce')\n",
    "        \n",
    "        print(f\"ğŸ¯ Average Match Score: {df['Score_Numeric'].mean():.1f}/100\")\n",
    "        print(f\"ğŸ† Highest Score: {df['Score_Numeric'].max():.1f}/100\")\n",
    "        print(f\"ğŸ“Š Score Range: {df['Score_Numeric'].min():.1f} - {df['Score_Numeric'].max():.1f}\")\n",
    "        print()\n",
    "        \n",
    "        excellent = len(df[df['Score_Numeric'] >= 80])\n",
    "        good = len(df[(df['Score_Numeric'] >= 60) & (df['Score_Numeric'] < 80)])\n",
    "        average = len(df[(df['Score_Numeric'] >= 40) & (df['Score_Numeric'] < 60)])\n",
    "        low = len(df[df['Score_Numeric'] < 40])\n",
    "        \n",
    "        print(\"ğŸ“ˆ SCORE DISTRIBUTION:\")\n",
    "        print(f\"   ğŸ¯ Excellent (80-100): {excellent} jobs\")\n",
    "        print(f\"   âœ… Good (60-79): {good} jobs\")\n",
    "        print(f\"   âš ï¸  Average (40-59): {average} jobs\")\n",
    "        print(f\"   ğŸ”´ Low (0-39): {low} jobs\")\n",
    "        print()\n",
    "        \n",
    "        print(\"ğŸ¢ TOP COMPANIES:\")\n",
    "        company_counts = df['Company'].value_counts()\n",
    "        for company, count in company_counts.head(3).items():\n",
    "            avg_score = df[df['Company'] == company]['Score_Numeric'].mean()\n",
    "            print(f\"   â€¢ {company}: {count} jobs (avg: {avg_score:.1f}/100)\")\n",
    "        print()\n",
    "        \n",
    "        all_matching_skills = []\n",
    "        for skills in df['Matching Skills'].dropna():\n",
    "            if str(skills) != 'nan':\n",
    "                all_matching_skills.extend([s.strip() for s in str(skills).split(',')])\n",
    "        \n",
    "        if all_matching_skills:\n",
    "            from collections import Counter\n",
    "            skill_counts = Counter(all_matching_skills)\n",
    "            print(\"ğŸ› ï¸  TOP MATCHING SKILLS:\")\n",
    "            for skill, count in skill_counts.most_common(5):\n",
    "                print(f\"   â€¢ {skill}: {count} jobs\")\n",
    "    \n",
    "        all_missing_skills = []\n",
    "        for skills in df['Missing Skills'].dropna():\n",
    "            if str(skills) != 'nan' and skills.strip():\n",
    "                all_missing_skills.extend([s.strip() for s in str(skills).split(',')])\n",
    "        \n",
    "        if all_missing_skills:\n",
    "            missing_counts = Counter(all_missing_skills)\n",
    "            print(\"\\nğŸ“š SKILLS TO IMPROVE:\")\n",
    "            for skill, count in missing_counts.most_common(3):\n",
    "                print(f\"   â€¢ {skill}: missing in {count} jobs\")\n",
    "        \n",
    "        print(\"\\nğŸ’¡ ACTION PLAN:\")\n",
    "        high_score_jobs = df[df['Score_Numeric'] >= 70]\n",
    "        if len(high_score_jobs) > 0:\n",
    "            print(f\"   ğŸ¯ Apply to {len(high_score_jobs)} high-match jobs first\")\n",
    "        else:\n",
    "            print(f\"   ğŸ¯ Apply to top {min(3, len(df))} jobs based on score\")\n",
    "        \n",
    "        if all_missing_skills:\n",
    "            top_missing = missing_counts.most_common(1)[0][0]\n",
    "            print(f\"   ğŸ“š Focus on learning: {top_missing}\")\n",
    "        \n",
    "        print(f\"   â° Apply within 48 hours for best results\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error generating insights: {e}\")\n",
    "\n",
    "def run_enhanced_job_search():\n",
    "    \n",
    "    YOUR_RESUME_PATH = \"/Users/siddharthshailendra/Resume_Shortlister_using_llm_basic/sample_resume_2.txt\"\n",
    "    JOB_TITLE = \"machine learning engineer\"  \n",
    "    LOCATION = \"Mumbai\" \n",
    "    \n",
    "    print(\"ğŸš€ ENHANCED JOB SEARCH STARTING...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    ranked_jobs, resume_data = find_relevant_jobs(\n",
    "        YOUR_RESUME_PATH,\n",
    "        JOB_TITLE, \n",
    "        LOCATION\n",
    "    )\n",
    "    \n",
    "    if ranked_jobs:\n",
    "        print(\"âœ… Jobs analyzed successfully!\")\n",
    "        save_results_to_csv(ranked_jobs, \"my_job_recommendations.csv\")\n",
    "        \n",
    "        # Display results in clean format\n",
    "        display_csv_results_clean(\"my_job_recommendations.csv\")\n",
    "        \n",
    "        # Show insights\n",
    "        display_job_search_insights(\"my_job_recommendations.csv\")\n",
    "        \n",
    "        print(f\"\\nğŸ‰ SEARCH COMPLETED! Found {len(ranked_jobs)} relevant jobs!\")\n",
    "        print(\"\\nğŸ’¼ NEXT STEPS:\")\n",
    "        print(\"   1. Apply to top 3 jobs today\")\n",
    "        print(\"   2. Customize cover letters for each application\")\n",
    "        print(\"   3. Follow up in 5-7 days\")\n",
    "        print(\"   4. Track applications in a spreadsheet\")\n",
    "        \n",
    "    else:\n",
    "        print(\"âŒ No relevant jobs found.\")\n",
    "\n",
    "# Also add this function to view existing CSV files nicely\n",
    "def view_existing_jobs_clean(csv_file_path=\"my_job_recommendations.csv\"):\n",
    "    print(f\"ğŸ“ Viewing: {csv_file_path}\")\n",
    "    print(\"=\" * 70)\n",
    "    display_csv_results_clean(csv_file_path)\n",
    "    display_job_search_insights(csv_file_path)\n",
    "run_enhanced_job_search()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
